{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPF0FptzZRCqdwHFIW2yyGx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plaban1981/indic_language/blob/main/Indic_language_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6iBgUU8ux77",
        "outputId": "d669a5f2-cb8d-4239-fc90-87ee8cdb8286"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNnXTHa-2kJT",
        "outputId": "16a5ca47-fd81-481e-873c-f0f2decbd08d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m1.2/1.3 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.move(\"/content/model_files/model_configs\",\"/content\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cLQny0geOrIW",
        "outputId": "8484a3e0-c3ff-430f-8e17-98086b7fc2e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/model_configs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.move(\"/content/model_configs\",\"/content/model_files/en-indic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Jg9rGGzTUURz",
        "outputId": "e06b406d-e4de-414f-a2b6-f0dcc8953585"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/model_files/en-indic/model_configs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the repo for running evaluation\n",
        "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
        "%cd indicTrans\n",
        "# clone requirements repositories\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone https://github.com/rsennrich/subword-nmt.git\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pLghtREMa0m",
        "outputId": "d5d0e8d8-8cf5-44c6-97bd-494a08637ff5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'indicTrans'...\n",
            "remote: Enumerating objects: 694, done.\u001b[K\n",
            "remote: Counting objects: 100% (397/397), done.\u001b[K\n",
            "remote: Compressing objects: 100% (201/201), done.\u001b[K\n",
            "remote: Total 694 (delta 273), reused 272 (delta 194), pack-reused 297\u001b[K\n",
            "Receiving objects: 100% (694/694), 2.65 MiB | 24.00 MiB/s, done.\n",
            "Resolving deltas: 100% (400/400), done.\n",
            "/content/indicTrans\n",
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1325, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 1325 (delta 91), reused 82 (delta 82), pack-reused 1218\u001b[K\n",
            "Receiving objects: 100% (1325/1325), 9.55 MiB | 14.05 MiB/s, done.\n",
            "Resolving deltas: 100% (701/701), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (139/139), 149.77 MiB | 16.81 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 597, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 597 (delta 8), reused 12 (delta 4), pack-reused 576\u001b[K\n",
            "Receiving objects: 100% (597/597), 252.23 KiB | 14.01 MiB/s, done.\n",
            "Resolving deltas: 100% (357/357), done.\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rm -rf \"/content/indicTrans\""
      ],
      "metadata": {
        "id": "jLV42clPO5IP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "sacremoses\n",
        "pandas\n",
        "mock\n",
        "sacrebleu\n",
        "pyarrow\n",
        "indic-nlp-library\n",
        "mosestokenizer\n",
        "subword-nmt\n",
        "numpy\n",
        "tensorboardX\n",
        "git+https://github.com/pytorch/fairseq.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tACp-xKX9Yh9",
        "outputId": "a849f6dc-c0e4-489a-f518-d287d5ad028e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fl6w6RVYo_K",
        "outputId": "2bdf7d66-447e-480a-abfa-f22a9856a564"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Mar 13 14:32:10 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0    24W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Kx-rCKaX9d7J",
        "outputId": "750e697a-c9eb-4ac6-b480-0acc03b8610f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/pytorch/fairseq.git (from -r requirements.txt (line 11))\n",
            "  Cloning https://github.com/pytorch/fairseq.git to /tmp/pip-req-build-q4vuzuwu\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pytorch/fairseq.git /tmp/pip-req-build-q4vuzuwu\n",
            "  Resolved https://github.com/pytorch/fairseq.git to commit 0338cdc3094ca7d29ff4d36d64791f7b4e4b5e6e\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  From https://github.com/ngoyal2707/Megatron-LM\n",
            "   * branch            adb23324c222aad0aad89308e70302d996a5eaeb -> FETCH_HEAD\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 2)) (1.3.5)\n",
            "Collecting mock\n",
            "  Downloading mock-5.0.1-py3-none-any.whl (30 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 5)) (9.0.0)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.81-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mosestokenizer\n",
            "  Downloading mosestokenizer-1.2.1.tar.gz (37 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 9)) (1.22.4)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from sacremoses->-r requirements.txt (line 1)) (2022.6.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sacremoses->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->-r requirements.txt (line 1)) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sacremoses->-r requirements.txt (line 1)) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas->-r requirements.txt (line 2)) (2022.7.1)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from sacrebleu->-r requirements.txt (line 4)) (4.9.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.9/dist-packages (from sacrebleu->-r requirements.txt (line 4)) (0.8.10)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Collecting sphinx-argparse\n",
            "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme\n",
            "  Downloading sphinx_rtd_theme-1.2.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openfile\n",
            "  Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
            "Collecting uctools\n",
            "  Downloading uctools-1.3.0.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting toolwrapper\n",
            "  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardX->-r requirements.txt (line 10)) (3.19.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorboardX->-r requirements.txt (line 10)) (23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.2->-r requirements.txt (line 11)) (1.2.1)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.2->-r requirements.txt (line 11)) (1.15.1)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.2->-r requirements.txt (line 11)) (0.29.33)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.2->-r requirements.txt (line 11)) (0.13.1+cu116)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.9/dist-packages (from fairseq==0.12.2->-r requirements.txt (line 11)) (1.13.1+cu116)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.6/269.6 KB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 KB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.9/dist-packages (from omegaconf<2.1->fairseq==0.12.2->-r requirements.txt (line 11)) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from omegaconf<2.1->fairseq==0.12.2->-r requirements.txt (line 11)) (4.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi->fairseq==0.12.2->-r requirements.txt (line 11)) (2.21)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->fairseq==0.12.2->-r requirements.txt (line 11)) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->fairseq==0.12.2->-r requirements.txt (line 11)) (3.1.0)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (3.5.4)\n",
            "Collecting sphinxcontrib-jquery!=3.0.0,>=2.0.0\n",
            "  Downloading sphinxcontrib_jquery-2.0.0-py3-none-any.whl (3.2 kB)\n",
            "Requirement already satisfied: docutils<0.19 in /usr/local/lib/python3.9/dist-packages (from sphinx-rtd-theme->indic-nlp-library->-r requirements.txt (line 6)) (0.16)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.9/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.9/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (2.0.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.9/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (2.2.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.9/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.9/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (1.0.4)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.9/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (2.12.1)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.9/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (57.4.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.9/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (1.0.3)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.9/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (0.7.13)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.9/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (2.6.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.9/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (2.25.1)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.9/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (3.1.2)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.9/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->-r requirements.txt (line 6)) (2.10)\n",
            "Building wheels for collected packages: sacremoses, mosestokenizer, fairseq, antlr4-python3-runtime, docopt, toolwrapper, uctools\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=ccee5d296f89baacbcaf638cf56030d0cca272fb95664cb4bfd2594de5c045bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
            "  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mosestokenizer: filename=mosestokenizer-1.2.1-py3-none-any.whl size=49188 sha256=e6fce55a7df09f70c1b9295676124a31acc8bc7335d67d591e12a32ec99502fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/1d/ca/6fb2622d3cec30ae4142be1e1ece4f879ef5efadd6a25b75f0\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp39-cp39-linux_x86_64.whl size=19485799 sha256=8a2e000e1174cbb88885bc139d6508a6af85ef16968aab29e08fb8dc33f8b6c1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bc44_8va/wheels/c4/c5/76/eaaaea25f63048f4b5f6bfa76172eb2d2e79cb4bf559c7ad2e\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141231 sha256=6341882e246b70cff0acfda859e03656070ee298c2a7b610b02394b8613ab90b\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/3c/ae/14db087e6018de74810afe32eb6ac890ef9c68ba19b00db97a\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=534a509020916efc4bba1d61ae6a7f4654105ccf69ad5ae033488aeb2cd02d89\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
            "  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3353 sha256=cb294538565b41288f22e431f46caf98c1d8661ae240ae0a2d94167ce1154f4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/fb/ad/2b280cddd52c15c21ac9599c2661fe8aec93778fb35da77c7f\n",
            "  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6162 sha256=bc0fd3ddabffe882d34147408a0e0fa12effb67e7c410773b6ee1e7e93be9412\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/34/ff/68e3afc33b6758aeef228ea97955d8f79f125b91d9d45a09ae\n",
            "Successfully built sacremoses mosestokenizer fairseq antlr4-python3-runtime docopt toolwrapper uctools\n",
            "Installing collected packages: toolwrapper, openfile, morfessor, docopt, bitarray, antlr4-python3-runtime, uctools, tensorboardX, sphinxcontrib-jquery, sacremoses, portalocker, omegaconf, mock, colorama, subword-nmt, sacrebleu, mosestokenizer, hydra-core, sphinx-rtd-theme, sphinx-argparse, fairseq, indic-nlp-library\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.7.3 colorama-0.4.6 docopt-0.6.2 fairseq-0.12.2 hydra-core-1.0.7 indic-nlp-library-0.81 mock-5.0.1 morfessor-2.0.6 mosestokenizer-1.2.1 omegaconf-2.0.6 openfile-0.0.7 portalocker-2.7.0 sacrebleu-2.3.1 sacremoses-0.0.53 sphinx-argparse-0.4.0 sphinx-rtd-theme-1.2.0 sphinxcontrib-jquery-2.0.0 subword-nmt-0.3.8 tensorboardX-2.6 toolwrapper-2.1.0 uctools-1.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download the indictrans model\n",
        "\n",
        "\n",
        "#### downloading the indic-en model\n",
        "\n",
        "!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/indic2en.zip\n",
        "\n",
        "!unzip indic2en.zip\n",
        "\n",
        "#### downloading the en-indic model\n",
        "!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/en2indic.zip\n",
        "\n",
        "!unzip en2indic.zip\n",
        "\n",
        "#### downloading the indic-indic model\n",
        "!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.zip\n",
        "\n",
        "!unzip m2m.zip"
      ],
      "metadata": {
        "id": "XIQnGaAJ9ux_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## English to Indic Languages"
      ],
      "metadata": {
        "id": "zKTXNUXo-avR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCOYZyvsY1GK",
        "outputId": "f29f2a5e-2a1f-424f-e65f-4d3fab9147e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir model_files"
      ],
      "metadata": {
        "id": "yJfMZEDJ_YFo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.move(\"/content/indicTrans/model_configs\",\"/content/drive/MyDrive/indic/model_files/en-indic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HhEIrRwYZeKs",
        "outputId": "c05658ea-3cd1-4ec4-f5bf-d1c0ada5fa8a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/indic/model_files/en-indic/model_configs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/en2indic.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_wZ80zAHwey",
        "outputId": "dfce25d4-3ca6-4aab-d174-b0948c1579de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-13 13:19:07--  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/en2indic.zip\n",
            "Resolving ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)... 164.52.206.154, 101.53.136.19, 101.53.136.18, ...\n",
            "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|164.52.206.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4811880516 (4.5G) [application/zip]\n",
            "Saving to: ‘en2indic.zip’\n",
            "\n",
            "en2indic.zip        100%[===================>]   4.48G  12.4MB/s    in 6m 46s  \n",
            "\n",
            "2023-03-13 13:25:55 (11.3 MB/s) - ‘en2indic.zip’ saved [4811880516/4811880516]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/en2indic.zip\" -d \"/content/model_files\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuCNV6lBHycI",
        "outputId": "548b9d98-b5ea-4482-bcde-50f2b76bfe43"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/en2indic.zip\n",
            "   creating: /content/model_files/en-indic/\n",
            "   creating: /content/model_files/en-indic/vocab/\n",
            "  inflating: /content/model_files/en-indic/vocab/bpe_codes.32k.SRC  \n",
            "  inflating: /content/model_files/en-indic/vocab/vocab.SRC  \n",
            "  inflating: /content/model_files/en-indic/vocab/vocab.TGT  \n",
            "  inflating: /content/model_files/en-indic/vocab/bpe_codes.32k.TGT  \n",
            "   creating: /content/model_files/en-indic/final_bin/\n",
            "  inflating: /content/model_files/en-indic/final_bin/preprocess.log  \n",
            "  inflating: /content/model_files/en-indic/final_bin/dict.TGT.txt  \n",
            "  inflating: /content/model_files/en-indic/final_bin/test.SRC-TGT.SRC.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/test.SRC-TGT.TGT.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/train.SRC-TGT.SRC.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/dict.SRC.txt  \n",
            "  inflating: /content/model_files/en-indic/final_bin/valid.SRC-TGT.TGT.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/test.SRC-TGT.TGT.bin  \n",
            "  inflating: /content/model_files/en-indic/final_bin/valid.SRC-TGT.TGT.bin  \n",
            "  inflating: /content/model_files/en-indic/final_bin/train.SRC-TGT.TGT.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/train.SRC-TGT.TGT.bin  \n",
            "  inflating: /content/model_files/en-indic/final_bin/valid.SRC-TGT.SRC.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/train.SRC-TGT.SRC.bin  \n",
            "  inflating: /content/model_files/en-indic/final_bin/valid.SRC-TGT.SRC.bin  \n",
            "  inflating: /content/model_files/en-indic/final_bin/test.SRC-TGT.SRC.bin  \n",
            "   creating: /content/model_files/en-indic/model/\n",
            "  inflating: /content/model_files/en-indic/model/checkpoint_best.pt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/indic2en.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXfB_7DoUQHC",
        "outputId": "83b72d89-c2eb-426d-adbd-0c214b83ccd8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-13 12:11:38--  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/indic2en.zip\n",
            "Resolving ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)... 164.52.206.155, 164.52.210.57, 101.53.152.30, ...\n",
            "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|164.52.206.155|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4759117228 (4.4G) [application/zip]\n",
            "Saving to: ‘indic2en.zip.1’\n",
            "\n",
            "indic2en.zip.1      100%[===================>]   4.43G  13.3MB/s    in 5m 56s  \n",
            "\n",
            "2023-03-13 12:17:36 (12.7 MB/s) - ‘indic2en.zip.1’ saved [4759117228/4759117228]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip indic2en.zip -d /content/model_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmTwQYnSXA4w",
        "outputId": "7aa24761-faa2-4803-b031-a619c76135e1"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  indic2en.zip\n",
            "   creating: /content/model_files/indic-en/\n",
            "   creating: /content/model_files/indic-en/vocab/\n",
            "  inflating: /content/model_files/indic-en/vocab/bpe_codes.32k.SRC  \n",
            "  inflating: /content/model_files/indic-en/vocab/vocab.SRC  \n",
            "  inflating: /content/model_files/indic-en/vocab/vocab.TGT  \n",
            "  inflating: /content/model_files/indic-en/vocab/bpe_codes.32k.TGT  \n",
            "   creating: /content/model_files/indic-en/final_bin/\n",
            "  inflating: /content/model_files/indic-en/final_bin/preprocess.log  \n",
            "  inflating: /content/model_files/indic-en/final_bin/dict.TGT.txt  \n",
            "  inflating: /content/model_files/indic-en/final_bin/test.SRC-TGT.SRC.idx  \n",
            "  inflating: /content/model_files/indic-en/final_bin/test.SRC-TGT.TGT.idx  \n",
            "  inflating: /content/model_files/indic-en/final_bin/train.SRC-TGT.SRC.idx  \n",
            "  inflating: /content/model_files/indic-en/final_bin/dict.SRC.txt  \n",
            "  inflating: /content/model_files/indic-en/final_bin/valid.SRC-TGT.TGT.idx  \n",
            "  inflating: /content/model_files/indic-en/final_bin/test.SRC-TGT.TGT.bin  \n",
            "  inflating: /content/model_files/indic-en/final_bin/valid.SRC-TGT.TGT.bin  \n",
            "  inflating: /content/model_files/indic-en/final_bin/train.SRC-TGT.TGT.idx  \n",
            "  inflating: /content/model_files/indic-en/final_bin/train.SRC-TGT.TGT.bin  \n",
            "  inflating: /content/model_files/indic-en/final_bin/valid.SRC-TGT.SRC.idx  \n",
            "  inflating: /content/model_files/indic-en/final_bin/train.SRC-TGT.SRC.bin  \n",
            "  inflating: /content/model_files/indic-en/final_bin/valid.SRC-TGT.SRC.bin  \n",
            "  inflating: /content/model_files/indic-en/final_bin/test.SRC-TGT.SRC.bin  \n",
            "   creating: /content/model_files/indic-en/model/\n",
            "  inflating: /content/model_files/indic-en/model/checkpoint_best.pt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/en-indic.zip -d /content/model_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXQV8YCVBAhQ",
        "outputId": "721c859b-cd09-4d49-d981-2d748c16595d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/en-indic.zip\n",
            "   creating: /content/model_files/en-indic/\n",
            "   creating: /content/model_files/en-indic/vocab/\n",
            "  inflating: /content/model_files/en-indic/vocab/bpe_codes.32k.SRC  \n",
            "  inflating: /content/model_files/en-indic/vocab/vocab.SRC  \n",
            "  inflating: /content/model_files/en-indic/vocab/vocab.TGT  \n",
            "  inflating: /content/model_files/en-indic/vocab/bpe_codes.32k.TGT  \n",
            "   creating: /content/model_files/en-indic/final_bin/\n",
            "  inflating: /content/model_files/en-indic/final_bin/preprocess.log  \n",
            "  inflating: /content/model_files/en-indic/final_bin/dict.TGT.txt  \n",
            "  inflating: /content/model_files/en-indic/final_bin/test.SRC-TGT.SRC.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/test.SRC-TGT.TGT.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/train.SRC-TGT.SRC.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/dict.SRC.txt  \n",
            "  inflating: /content/model_files/en-indic/final_bin/valid.SRC-TGT.TGT.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/test.SRC-TGT.TGT.bin  \n",
            "  inflating: /content/model_files/en-indic/final_bin/valid.SRC-TGT.TGT.bin  \n",
            "  inflating: /content/model_files/en-indic/final_bin/train.SRC-TGT.TGT.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/train.SRC-TGT.TGT.bin  \n",
            "  inflating: /content/model_files/en-indic/final_bin/valid.SRC-TGT.SRC.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/train.SRC-TGT.SRC.bin  \n",
            "  inflating: /content/model_files/en-indic/final_bin/valid.SRC-TGT.SRC.bin  \n",
            "  inflating: /content/model_files/en-indic/final_bin/test.SRC-TGT.SRC.bin  \n",
            "   creating: /content/model_files/en-indic/model/\n",
            "  inflating: /content/model_files/en-indic/model/checkpoint_best.pt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the repo for running evaluation\n",
        "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
        "%cd indicTrans\n",
        "# clone requirements repositories\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone https://github.com/rsennrich/subword-nmt.git\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stjQDuX3EPXU",
        "outputId": "884ac372-6b2e-4e04-c822-4df89b8e1c1d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'indicTrans'...\n",
            "remote: Enumerating objects: 694, done.\u001b[K\n",
            "remote: Counting objects: 100% (397/397), done.\u001b[K\n",
            "remote: Compressing objects: 100% (201/201), done.\u001b[K\n",
            "remote: Total 694 (delta 273), reused 272 (delta 194), pack-reused 297\u001b[K\n",
            "Receiving objects: 100% (694/694), 2.65 MiB | 14.27 MiB/s, done.\n",
            "Resolving deltas: 100% (400/400), done.\n",
            "/content/indicTrans\n",
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1325, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 1325 (delta 91), reused 82 (delta 82), pack-reused 1218\u001b[K\n",
            "Receiving objects: 100% (1325/1325), 9.55 MiB | 9.98 MiB/s, done.\n",
            "Resolving deltas: 100% (701/701), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (139/139), 149.77 MiB | 24.62 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Updating files: 100% (28/28), done.\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 597, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 597 (delta 8), reused 12 (delta 4), pack-reused 576\u001b[K\n",
            "Receiving objects: 100% (597/597), 252.23 KiB | 4.35 MiB/s, done.\n",
            "Resolving deltas: 100% (357/357), done.\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "flCDebB8GHkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Translate raw text with a trained model. Batches data on-the-fly.\n",
        "\"\"\"\n",
        "\n",
        "import ast\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "from fairseq import checkpoint_utils, options, tasks, utils\n",
        "from fairseq.dataclass.utils import convert_namespace_to_omegaconf\n",
        "from fairseq.token_generation_constraints import pack_constraints, unpack_constraints\n",
        "from fairseq_cli.generate import get_symbols_to_strip_from_output\n",
        "\n",
        "import codecs\n",
        "\n",
        "\n",
        "Batch = namedtuple(\"Batch\", \"ids src_tokens src_lengths constraints\")\n",
        "Translation = namedtuple(\"Translation\", \"src_str hypos pos_scores alignments\")\n",
        "\n",
        "\n",
        "def make_batches(\n",
        "    lines, cfg, task, max_positions, encode_fn, constrainted_decoding=False\n",
        "):\n",
        "    def encode_fn_target(x):\n",
        "        return encode_fn(x)\n",
        "\n",
        "    if constrainted_decoding:\n",
        "        # Strip (tab-delimited) contraints, if present, from input lines,\n",
        "        # store them in batch_constraints\n",
        "        batch_constraints = [list() for _ in lines]\n",
        "        for i, line in enumerate(lines):\n",
        "            if \"\\t\" in line:\n",
        "                lines[i], *batch_constraints[i] = line.split(\"\\t\")\n",
        "\n",
        "        # Convert each List[str] to List[Tensor]\n",
        "        for i, constraint_list in enumerate(batch_constraints):\n",
        "            batch_constraints[i] = [\n",
        "                task.target_dictionary.encode_line(\n",
        "                    encode_fn_target(constraint),\n",
        "                    append_eos=False,\n",
        "                    add_if_not_exist=False,\n",
        "                )\n",
        "                for constraint in constraint_list\n",
        "            ]\n",
        "\n",
        "    if constrainted_decoding:\n",
        "        constraints_tensor = pack_constraints(batch_constraints)\n",
        "    else:\n",
        "        constraints_tensor = None\n",
        "\n",
        "    tokens, lengths = task.get_interactive_tokens_and_lengths(lines, encode_fn)\n",
        "\n",
        "    itr = task.get_batch_iterator(\n",
        "        dataset=task.build_dataset_for_inference(\n",
        "            tokens, lengths, constraints=constraints_tensor\n",
        "        ),\n",
        "        max_tokens=cfg.dataset.max_tokens,\n",
        "        max_sentences=cfg.dataset.batch_size,\n",
        "        max_positions=max_positions,\n",
        "        ignore_invalid_inputs=cfg.dataset.skip_invalid_size_inputs_valid_test,\n",
        "    ).next_epoch_itr(shuffle=False)\n",
        "    for batch in itr:\n",
        "        ids = batch[\"id\"]\n",
        "        src_tokens = batch[\"net_input\"][\"src_tokens\"]\n",
        "        src_lengths = batch[\"net_input\"][\"src_lengths\"]\n",
        "        constraints = batch.get(\"constraints\", None)\n",
        "\n",
        "        yield Batch(\n",
        "            ids=ids,\n",
        "            src_tokens=src_tokens,\n",
        "            src_lengths=src_lengths,\n",
        "            constraints=constraints,\n",
        "        )\n",
        "\n",
        "\n",
        "class Translator:\n",
        "    def __init__(\n",
        "        self, model_path,data_dir, checkpoint_path, batch_size=25, constrained_decoding=False\n",
        "    ):\n",
        "\n",
        "        self.constrained_decoding = constrained_decoding\n",
        "        self.parser = options.get_generation_parser(interactive=True)\n",
        "        self.model_path = model_path\n",
        "        # buffer_size is currently not used but we just initialize it to batch\n",
        "        # size + 1 to avoid any assertion errors.\n",
        "        if self.constrained_decoding:\n",
        "            self.parser.set_defaults(\n",
        "                path=checkpoint_path,\n",
        "                remove_bpe=\"subword_nmt\",\n",
        "                num_workers=-1,\n",
        "                constraints=\"ordered\",\n",
        "                batch_size=batch_size,\n",
        "                buffer_size=batch_size + 1,\n",
        "            )\n",
        "        else:\n",
        "            self.parser.set_defaults(\n",
        "                path=checkpoint_path,\n",
        "                remove_bpe=\"subword_nmt\",\n",
        "                num_workers=-1,\n",
        "                batch_size=batch_size,\n",
        "                buffer_size=batch_size + 1,\n",
        "            )\n",
        "        args = options.parse_args_and_arch(self.parser, input_args=[data_dir])\n",
        "        # we are explictly setting src_lang and tgt_lang here\n",
        "        # generally the data_dir we pass contains {split}-{src_lang}-{tgt_lang}.*.idx files from\n",
        "        # which fairseq infers the src and tgt langs(if these are not passed). In deployment we dont\n",
        "        # use any idx files and only store the SRC and TGT dictionaries.\n",
        "        args.source_lang = \"SRC\"\n",
        "        args.target_lang = \"TGT\"\n",
        "        # since we are truncating sentences to max_seq_len in engine, we can set it to False here\n",
        "        args.skip_invalid_size_inputs_valid_test = False\n",
        "\n",
        "        # we have custom architechtures in this folder and we will let fairseq\n",
        "        # import this\n",
        "        model_file_path = os.path.join(self.model_path,\"en-indic\",\"model_configs\")\n",
        "        args.user_dir = \"model_configs\"\n",
        "        self.cfg = convert_namespace_to_omegaconf(args)\n",
        "\n",
        "        utils.import_user_module(self.cfg.common)\n",
        "\n",
        "        if self.cfg.interactive.buffer_size < 1:\n",
        "            self.cfg.interactive.buffer_size = 1\n",
        "        if self.cfg.dataset.max_tokens is None and self.cfg.dataset.batch_size is None:\n",
        "            self.cfg.dataset.batch_size = 1\n",
        "\n",
        "        assert (\n",
        "            not self.cfg.generation.sampling\n",
        "            or self.cfg.generation.nbest == self.cfg.generation.beam\n",
        "        ), \"--sampling requires --nbest to be equal to --beam\"\n",
        "        assert (\n",
        "            not self.cfg.dataset.batch_size\n",
        "            or self.cfg.dataset.batch_size <= self.cfg.interactive.buffer_size\n",
        "        ), \"--batch-size cannot be larger than --buffer-size\"\n",
        "\n",
        "        # Fix seed for stochastic decoding\n",
        "        # if self.cfg.common.seed is not None and not self.cfg.generation.no_seed_provided:\n",
        "        #     np.random.seed(self.cfg.common.seed)\n",
        "        #     utils.set_torch_seed(self.cfg.common.seed)\n",
        "\n",
        "        # if not self.constrained_decoding:\n",
        "        #     self.use_cuda = torch.cuda.is_available() and not self.cfg.common.cpu\n",
        "        # else:\n",
        "        #     self.use_cuda = False\n",
        "\n",
        "        #self.use_cuda = torch.cuda.is_available() and not self.cfg.common.cpu\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        # Setup task, e.g., translation\n",
        "        self.task = tasks.setup_task(self.cfg.task)\n",
        "\n",
        "        # Load ensemble\n",
        "        overrides = ast.literal_eval(self.cfg.common_eval.model_overrides)\n",
        "        self.models, self._model_args = checkpoint_utils.load_model_ensemble(\n",
        "            utils.split_paths(self.cfg.common_eval.path),\n",
        "            arg_overrides=overrides,\n",
        "            task=self.task,\n",
        "            suffix=self.cfg.checkpoint.checkpoint_suffix,\n",
        "            strict=(self.cfg.checkpoint.checkpoint_shard_count == 1),\n",
        "            num_shards=self.cfg.checkpoint.checkpoint_shard_count,\n",
        "        )\n",
        "\n",
        "        # Set dictionaries\n",
        "        self.src_dict = self.task.source_dictionary\n",
        "        self.tgt_dict = self.task.target_dictionary\n",
        "\n",
        "        # Optimize ensemble for generation\n",
        "        for model in self.models:\n",
        "            if model is None:\n",
        "                continue\n",
        "            if self.cfg.common.fp16:\n",
        "                model.half()\n",
        "            if (\n",
        "                self.use_cuda\n",
        "                and not self.cfg.distributed_training.pipeline_model_parallel\n",
        "            ):\n",
        "                model.cuda()\n",
        "            model.prepare_for_inference_(self.cfg)\n",
        "\n",
        "        # Initialize generator\n",
        "        self.generator = self.task.build_generator(self.models, self.cfg.generation)\n",
        "\n",
        "        # Handle tokenization and BPE\n",
        "        self.tokenizer = self.task.build_tokenizer(self.cfg.tokenizer)\n",
        "        self.bpe = self.task.build_bpe(self.cfg.bpe)\n",
        "\n",
        "        # Load alignment dictionary for unknown word replacement\n",
        "        # (None if no unknown word replacement, empty if no path to align dictionary)\n",
        "        self.align_dict = utils.load_align_dict(self.cfg.generation.replace_unk)\n",
        "\n",
        "        self.max_positions = utils.resolve_max_positions(\n",
        "            self.task.max_positions(), *[model.max_positions() for model in self.models]\n",
        "        )\n",
        "\n",
        "    def encode_fn(self, x):\n",
        "        if self.tokenizer is not None:\n",
        "            x = self.tokenizer.encode(x)\n",
        "        if self.bpe is not None:\n",
        "            x = self.bpe.encode(x)\n",
        "        return x\n",
        "\n",
        "    def decode_fn(self, x):\n",
        "        if self.bpe is not None:\n",
        "            x = self.bpe.decode(x)\n",
        "        if self.tokenizer is not None:\n",
        "            x = self.tokenizer.decode(x)\n",
        "        return x\n",
        "\n",
        "    def translate(self, inputs, constraints=None):\n",
        "        if self.constrained_decoding and constraints is None:\n",
        "            raise ValueError(\"Constraints cant be None in constrained decoding mode\")\n",
        "        if not self.constrained_decoding and constraints is not None:\n",
        "            raise ValueError(\"Cannot pass constraints during normal translation\")\n",
        "        if constraints:\n",
        "            constrained_decoding = True\n",
        "            modified_inputs = []\n",
        "            for _input, constraint in zip(inputs, constraints):\n",
        "                modified_inputs.append(_input + f\"\\t{constraint}\")\n",
        "            inputs = modified_inputs\n",
        "        else:\n",
        "            constrained_decoding = False\n",
        "\n",
        "        start_id = 0\n",
        "        results = []\n",
        "        final_translations = []\n",
        "        for batch in make_batches(\n",
        "            inputs,\n",
        "            self.cfg,\n",
        "            self.task,\n",
        "            self.max_positions,\n",
        "            self.encode_fn,\n",
        "            constrained_decoding,\n",
        "        ):\n",
        "            bsz = batch.src_tokens.size(0)\n",
        "            src_tokens = batch.src_tokens\n",
        "            src_lengths = batch.src_lengths\n",
        "            constraints = batch.constraints\n",
        "            if self.use_cuda:\n",
        "                src_tokens = src_tokens.cuda()\n",
        "                src_lengths = src_lengths.cuda()\n",
        "                if constraints is not None:\n",
        "                    constraints = constraints.cuda()\n",
        "\n",
        "            sample = {\n",
        "                \"net_input\": {\n",
        "                    \"src_tokens\": src_tokens,\n",
        "                    \"src_lengths\": src_lengths,\n",
        "                },\n",
        "            }\n",
        "\n",
        "            translations = self.task.inference_step(\n",
        "                self.generator, self.models, sample, constraints=constraints\n",
        "            )\n",
        "\n",
        "            list_constraints = [[] for _ in range(bsz)]\n",
        "            if constrained_decoding:\n",
        "                list_constraints = [unpack_constraints(c) for c in constraints]\n",
        "            for i, (id, hypos) in enumerate(zip(batch.ids.tolist(), translations)):\n",
        "                src_tokens_i = utils.strip_pad(src_tokens[i], self.tgt_dict.pad())\n",
        "                constraints = list_constraints[i]\n",
        "                results.append(\n",
        "                    (\n",
        "                        start_id + id,\n",
        "                        src_tokens_i,\n",
        "                        hypos,\n",
        "                        {\n",
        "                            \"constraints\": constraints,\n",
        "                        },\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        # sort output to match input order\n",
        "        for id_, src_tokens, hypos, _ in sorted(results, key=lambda x: x[0]):\n",
        "            src_str = \"\"\n",
        "            if self.src_dict is not None:\n",
        "                src_str = self.src_dict.string(\n",
        "                    src_tokens, self.cfg.common_eval.post_process\n",
        "                )\n",
        "\n",
        "            # Process top predictions\n",
        "            for hypo in hypos[: min(len(hypos), self.cfg.generation.nbest)]:\n",
        "                hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n",
        "                    hypo_tokens=hypo[\"tokens\"].int().cpu(),\n",
        "                    src_str=src_str,\n",
        "                    alignment=hypo[\"alignment\"],\n",
        "                    align_dict=self.align_dict,\n",
        "                    tgt_dict=self.tgt_dict,\n",
        "                    remove_bpe=\"subword_nmt\",\n",
        "                    extra_symbols_to_ignore=get_symbols_to_strip_from_output(\n",
        "                        self.generator\n",
        "                    ),\n",
        "                )\n",
        "                detok_hypo_str = self.decode_fn(hypo_str)\n",
        "                final_translations.append(detok_hypo_str)\n",
        "        return final_translations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rurJ73ISAPT",
        "outputId": "aece8506-8cc7-49a9-c547-7416b3f9d2d6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import truncate\n",
        "from sacremoses import MosesPunctNormalizer\n",
        "from sacremoses import MosesTokenizer\n",
        "from sacremoses import MosesDetokenizer\n",
        "from subword_nmt.apply_bpe import BPE, read_vocabulary\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "from indicnlp.tokenize import indic_detokenize\n",
        "from indicnlp.normalize import indic_normalize\n",
        "from indicnlp.transliterate import unicode_transliterate\n",
        "from mosestokenizer import MosesSentenceSplitter\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "\n",
        "#from inference.custom_interactive import Translator\n",
        "#Translator = Translator()\n",
        "\n",
        "INDIC = [\"as\", \"bn\", \"gu\", \"hi\", \"kn\", \"ml\", \"mr\", \"or\", \"pa\", \"ta\", \"te\"]\n",
        "\n",
        "\n",
        "def split_sentences(paragraph, language):\n",
        "    if language == \"en\":\n",
        "        with MosesSentenceSplitter(language) as splitter:\n",
        "            return splitter([paragraph])\n",
        "    elif language in INDIC:\n",
        "        return sentence_tokenize.sentence_split(paragraph, lang=language)\n",
        "\n",
        "\n",
        "def add_token(sent, tag_infos):\n",
        "    \"\"\"add special tokens specified by tag_infos to each element in list\n",
        "    tag_infos: list of tuples (tag_type,tag)\n",
        "    each tag_info results in a token of the form: __{tag_type}__{tag}__\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = []\n",
        "    for tag_type, tag in tag_infos:\n",
        "        token = \"__\" + tag_type + \"__\" + tag + \"__\"\n",
        "        tokens.append(token)\n",
        "\n",
        "    return \" \".join(tokens) + \" \" + sent\n",
        "\n",
        "\n",
        "def apply_lang_tags(sents, src_lang, tgt_lang):\n",
        "    tagged_sents = []\n",
        "    for sent in sents:\n",
        "        tagged_sent = add_token(sent.strip(), [(\"src\", src_lang), (\"tgt\", tgt_lang)])\n",
        "        tagged_sents.append(tagged_sent)\n",
        "    return tagged_sents\n",
        "\n",
        "\n",
        "def truncate_long_sentences(sents):\n",
        "\n",
        "    MAX_SEQ_LEN = 200\n",
        "    new_sents = []\n",
        "\n",
        "    for sent in sents:\n",
        "        words = sent.split()\n",
        "        num_words = len(words)\n",
        "        if num_words > MAX_SEQ_LEN:\n",
        "            print_str = \" \".join(words[:5]) + \" .... \" + \" \".join(words[-5:])\n",
        "            sent = \" \".join(words[:MAX_SEQ_LEN])\n",
        "            print(\n",
        "                f\"WARNING: Sentence {print_str} truncated to 200 tokens as it exceeds maximum length limit\"\n",
        "            )\n",
        "\n",
        "        new_sents.append(sent)\n",
        "    return new_sents\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, expdir,model_path):\n",
        "        self.expdir = expdir\n",
        "        self.model_path = model_path\n",
        "        self.en_tok = MosesTokenizer(lang=\"en\")\n",
        "        self.en_normalizer = MosesPunctNormalizer()\n",
        "        self.en_detok = MosesDetokenizer(lang=\"en\")\n",
        "        self.xliterator = unicode_transliterate.UnicodeIndicTransliterator()\n",
        "        print(\"Initializing vocab and bpe\")\n",
        "        self.vocabulary = read_vocabulary(\n",
        "            codecs.open(f\"{expdir}/vocab/vocab.SRC\", encoding=\"utf-8\"), 5\n",
        "        )\n",
        "        self.bpe = BPE(\n",
        "            codecs.open(f\"{expdir}/vocab/bpe_codes.32k.SRC\", encoding=\"utf-8\"),\n",
        "            -1,\n",
        "            \"@@\",\n",
        "            self.vocabulary,\n",
        "            None,\n",
        "        )\n",
        "\n",
        "        print(\"Initializing model for translation\")\n",
        "        # initialize the model\n",
        "        self.translator = Translator(\n",
        "            model_path,f\"{expdir}/final_bin\", f\"{expdir}/model/checkpoint_best.pt\", batch_size=100\n",
        "        )\n",
        "\n",
        "    # translate a batch of sentences from src_lang to tgt_lang\n",
        "    def batch_translate(self, batch, src_lang, tgt_lang):\n",
        "\n",
        "        assert isinstance(batch, list)\n",
        "        preprocessed_sents = self.preprocess(batch, lang=src_lang)\n",
        "        bpe_sents = self.apply_bpe(preprocessed_sents)\n",
        "        tagged_sents = apply_lang_tags(bpe_sents, src_lang, tgt_lang)\n",
        "        tagged_sents = truncate_long_sentences(tagged_sents)\n",
        "\n",
        "        translations = self.translator.translate(tagged_sents)\n",
        "        postprocessed_sents = self.postprocess(translations, tgt_lang)\n",
        "\n",
        "        return postprocessed_sents\n",
        "\n",
        "    # translate a paragraph from src_lang to tgt_lang\n",
        "    def translate_paragraph(self, paragraph, src_lang, tgt_lang):\n",
        "\n",
        "        assert isinstance(paragraph, str)\n",
        "        sents = split_sentences(paragraph, src_lang)\n",
        "\n",
        "        postprocessed_sents = self.batch_translate(sents, src_lang, tgt_lang)\n",
        "\n",
        "        translated_paragraph = \" \".join(postprocessed_sents)\n",
        "\n",
        "        return translated_paragraph\n",
        "\n",
        "    def preprocess_sent(self, sent, normalizer, lang):\n",
        "        if lang == \"en\":\n",
        "            return \" \".join(\n",
        "                self.en_tok.tokenize(\n",
        "                    self.en_normalizer.normalize(sent.strip()), escape=False\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            # line = indic_detokenize.trivial_detokenize(line.strip(), lang)\n",
        "            return unicode_transliterate.UnicodeIndicTransliterator.transliterate(\n",
        "                \" \".join(\n",
        "                    indic_tokenize.trivial_tokenize(\n",
        "                        normalizer.normalize(sent.strip()), lang\n",
        "                    )\n",
        "                ),\n",
        "                lang,\n",
        "                \"hi\",\n",
        "            ).replace(\" ् \", \"्\")\n",
        "\n",
        "    def preprocess(self, sents, lang):\n",
        "        \"\"\"\n",
        "        Normalize, tokenize and script convert(for Indic)\n",
        "        return number of sentences input file\n",
        "        \"\"\"\n",
        "\n",
        "        if lang == \"en\":\n",
        "\n",
        "            # processed_sents = Parallel(n_jobs=-1, backend=\"multiprocessing\")(\n",
        "            #     delayed(preprocess_line)(line, None, lang) for line in tqdm(sents, total=num_lines)\n",
        "            # )\n",
        "            processed_sents = [\n",
        "                self.preprocess_sent(line, None, lang) for line in tqdm(sents)\n",
        "            ]\n",
        "\n",
        "        else:\n",
        "            normfactory = indic_normalize.IndicNormalizerFactory()\n",
        "            normalizer = normfactory.get_normalizer(lang)\n",
        "\n",
        "            # processed_sents = Parallel(n_jobs=-1, backend=\"multiprocessing\")(\n",
        "            #     delayed(preprocess_line)(line, normalizer, lang) for line in tqdm(infile, total=num_lines)\n",
        "            # )\n",
        "            processed_sents = [\n",
        "                self.preprocess_sent(line, normalizer, lang) for line in tqdm(sents)\n",
        "            ]\n",
        "\n",
        "        return processed_sents\n",
        "\n",
        "    def postprocess(self, sents, lang, common_lang=\"hi\"):\n",
        "        \"\"\"\n",
        "        parse fairseq interactive output, convert script back to native Indic script (in case of Indic languages) and detokenize.\n",
        "        infname: fairseq log file\n",
        "        outfname: output file of translation (sentences not translated contain the dummy string 'DUMMY_OUTPUT'\n",
        "        input_size: expected number of output sentences\n",
        "        lang: language\n",
        "        \"\"\"\n",
        "        postprocessed_sents = []\n",
        "\n",
        "        if lang == \"en\":\n",
        "            for sent in sents:\n",
        "                # outfile.write(en_detok.detokenize(sent.split(\" \")) + \"\\n\")\n",
        "                postprocessed_sents.append(self.en_detok.detokenize(sent.split(\" \")))\n",
        "        else:\n",
        "            for sent in sents:\n",
        "                outstr = indic_detokenize.trivial_detokenize(\n",
        "                    self.xliterator.transliterate(sent, common_lang, lang), lang\n",
        "                )\n",
        "                # outfile.write(outstr + \"\\n\")\n",
        "                postprocessed_sents.append(outstr)\n",
        "        return postprocessed_sents\n",
        "\n",
        "    def apply_bpe(self, sents):\n",
        "\n",
        "        return [self.bpe.process_line(sent) for sent in sents]\n"
      ],
      "metadata": {
        "id": "_-VxPXxzC36D"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd indicTrans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f1mg5-TEos6",
        "outputId": "d846585b-79d9-4421-9a4a-c3692bd5c9be"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/indicTrans\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils\n",
        "#from indicTrans.inference.engine import Model\n",
        "#from inference.engine import Model\n",
        "#\n",
        "indic2en_model = Model(expdir='/content/model_files/en-indic')\n",
        "#\n",
        "INDIC = {\"Assamese\": \"as\", \"Bengali\": \"bn\", \"Gujarati\": \"gu\", \"Hindi\": \"hi\",\"Kannada\": \"kn\",\"Malayalam\": \"ml\", \"Marathi\": \"mr\", \"Odia\": \"or\",\"Punjabi\": \"pa\",\"Tamil\": \"ta\", \"Telugu\" : \"te\"}\n",
        "#\n",
        "languages = list(INDIC.keys())\n",
        "#\n",
        "\n",
        "def translate(text, lang):\n",
        "  return indic2en_model.translate_paragraph(text, 'en', INDIC[lang])\n",
        "\n",
        "def translate_batch(text, lang):\n",
        "  return indic2en_model.batch_translate(text, 'en', INDIC[lang])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pERLFxk96dy3",
        "outputId": "728fe882-9249-49a9-cb7b-a8409f275c21"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing vocab and bpe\n",
            "Initializing model for translation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = [\"Elon Musk sells $8.5 billion in Tesla stock\", \n",
        "              \"I'm a professional academic and research writer.\",\n",
        "              \"Get a job in US and work in Germany\"]"
      ],
      "metadata": {
        "id": "kajTfvbRNnq2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in input_text:\n",
        "  print(translate(text, 'Tamil'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRy1zuTwBwKT",
        "outputId": "98cca5eb-f8c6-458e-fbe0-0087e901a21b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "டெஸ்லா நிறுவனத்தில் 8.5 பில்லியன் டாலர் பங்குகளை விற்றார் எலான் மஸ்க்\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 761.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "நான் ஒரு தொழில்முறை கல்வியாளர் மற்றும் ஆராய்ச்சி எழுத்தாளர்.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1321.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "அமெரிக்காவில் வேலை, ஜெர்மனியில் வேலை\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate_batch(input_text, 'Hindi')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD__5WfgQC75",
        "outputId": "4b2d9c70-b3e1-4b0c-f0d9-68b582bb8632"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 3139.45it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['एलन मस्क ने टेस्ला के 8.5 अरब डॉलर के शेयर बेचे',\n",
              " 'मैं एक पेशेवर अकादमिक और शोध लेखक हूं।',\n",
              " 'अमेरिका में नौकरी करो और जर्मनी में काम करो']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/input.txt\n",
        "The Sun Temple was built in the 13th century and designed as a gigantic chariot of the Sun God, Surya, with twelve pairs of ornamented wheels pulled by seven horses. Some of the wheels are 3 metres wide. Only six of the seven horse still stand today.The temple fell into disuse after an envoy of Jahangir desecrated the temple in the early 17th century.\n",
        "According to folklore, there was a diamond in the centre of the idol which reflected the sun rays that passed. In 1627, the then Raja of Khurda took the Sun idol from Konark to the Jagannath temple in Puri. The Sun temple belongs to the Kalingan school of Indian temple architecture. The alignment of the Sun Temple is along the east–west direction. The inner sanctum or vimana used to be surmounted by a tower or shikara but it was razed in the 19th century. The audience hall or jagamohana still stands and comprises majority of the ruins. The roof of the dance hall or natmandir has fallen off. It stands at the eastern end of the ruins on a raised platform.\n",
        "In 1559, Mukunda Gajapati came to throne in Cuttack. He aligned himself as an ally of Akbar and an enemy of the Sultan of Bengal, Sulaiman Khan Karrani. After a few battles, Odisha finally fell. The fall was also aided by the internal turmoil of the state. In 1568, the Konark temple was damaged by the army of Kalapahad, a general of the Sultan.Kalapahad is also said to be responsible for damages to several other temples during the conquest.;Odia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbBms6rfub6A",
        "outputId": "3374a396-836b-4422-ba77-3c98b9aa4d9e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/input.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/input.txt\",\"r\",encoding=\"utf-8\") as rd:\n",
        "  text = rd.read()\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nbncGJqu80T",
        "outputId": "ce26c839-77fd-4337-8ec2-e06f9fcbd9c4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Sun Temple was built in the 13th century and designed as a gigantic chariot of the Sun God, Surya, with twelve pairs of ornamented wheels pulled by seven horses. Some of the wheels are 3 metres wide. Only six of the seven horse still stand today.The temple fell into disuse after an envoy of Jahangir desecrated the temple in the early 17th century.\n",
            "According to folklore, there was a diamond in the centre of the idol which reflected the sun rays that passed. In 1627, the then Raja of Khurda took the Sun idol from Konark to the Jagannath temple in Puri. The Sun temple belongs to the Kalingan school of Indian temple architecture. The alignment of the Sun Temple is along the east–west direction. The inner sanctum or vimana used to be surmounted by a tower or shikara but it was razed in the 19th century. The audience hall or jagamohana still stands and comprises majority of the ruins. The roof of the dance hall or natmandir has fallen off. It stands at the eastern end of the ruins on a raised platform.\n",
            "In 1559, Mukunda Gajapati came to throne in Cuttack. He aligned himself as an ally of Akbar and an enemy of the Sultan of Bengal, Sulaiman Khan Karrani. After a few battles, Odisha finally fell. The fall was also aided by the internal turmoil of the state. In 1568, the Konark temple was damaged by the army of Kalapahad, a general of the Sultan.Kalapahad is also said to be responsible for damages to several other temples during the conquest.;Odia\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch = split_sentences(text, language=\"en\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "NPwKnqJ3uzaP",
        "outputId": "a167d4ea-410a-4795-abbd-40de45637ada"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-cd25bb19ee7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'split_sentences' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlaAhXDyvRXb",
        "outputId": "d385a454-d9eb-4e5e-f28c-4d91cc5f5b87"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The Sun Temple was built in the 13th century and designed as a gigantic chariot of the Sun God, Surya, with twelve pairs of ornamented wheels pulled by seven horses.',\n",
              " 'Some of the wheels are 3 metres wide.',\n",
              " 'Only six of the seven horse still stand today.The temple fell into disuse after an envoy of Jahangir desecrated the temple in the early 17th century.',\n",
              " 'According to folklore, there was a diamond in the centre of the idol which reflected the sun rays that passed.',\n",
              " 'In 1627, the then Raja of Khurda took the Sun idol from Konark to the Jagannath temple in Puri.',\n",
              " 'The Sun temple belongs to the Kalingan school of Indian temple architecture.',\n",
              " 'The alignment of the Sun Temple is along the east–west direction.',\n",
              " 'The inner sanctum or vimana used to be surmounted by a tower or shikara but it was razed in the 19th century.',\n",
              " 'The audience hall or jagamohana still stands and comprises majority of the ruins.',\n",
              " 'The roof of the dance hall or natmandir has fallen off.',\n",
              " 'It stands at the eastern end of the ruins on a raised platform.',\n",
              " 'In 1559, Mukunda Gajapati came to throne in Cuttack.',\n",
              " 'He aligned himself as an ally of Akbar and an enemy of the Sultan of Bengal, Sulaiman Khan Karrani.',\n",
              " 'After a few battles, Odisha finally fell.',\n",
              " 'The fall was also aided by the internal turmoil of the state.',\n",
              " 'In 1568, the Konark temple was damaged by the army of Kalapahad, a general of the Sultan.Kalapahad is also said to be responsible for damages to several other temples during the conquest.']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lang = 'Odia'\n",
        "\" \".join(indic2en_model.batch_translate(input_batch, 'en', INDIC[lang]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "2U9JPMB2vfPy",
        "outputId": "cc54d9f1-43c9-43dd-e134-b22a1a8a7e9b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 1627.83it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ତ୍ରୟୋଦଶ ଶତାବ୍ଦୀରେ ନିର୍ମିତ ଏହି ସୂର୍ଯ୍ୟମନ୍ଦିର ସୂର୍ଯ୍ୟଦେବଙ୍କ ଏକ ବିଶାଳ ରଥ ଭାବେ ପରିଗଣିତ ହୋଇଥିଲା। କେତେକ ଚକା 3 ମିଟର ଚଉଡ଼ା। ସପ୍ତଦଶ ଶତାବ୍ଦୀର ପ୍ରାରମ୍ଭରେ ଜାହାଙ୍ଗୀରଙ୍କ ଦୂତ ମନ୍ଦିରକୁ ଅପବିତ୍ର କରିବା ପରେ ଏହି ମନ୍ଦିର ନଷ୍ଟ ହୋଇଯାଇଥିଲା। ଲୋକକଥା ଅନୁସାରେ ମୂର୍ତ୍ତିର ମଧ୍ୟଭାଗରେ ଗୋଟିଏ ହୀରା ଥିଲା ଯାହା ସୂର୍ଯ୍ୟକିରଣ ପ୍ରତିଫଳିତ କରୁଥିଲା। 1627 ମସିହାରେ ତତ୍କାଳୀନ ଖୋର୍ଦ୍ଧା ରାଜା ସୂର୍ଯ୍ୟମୂର୍ତ୍ତିକୁ କୋଣାର୍କରୁ ପୁରୀର ଜଗନ୍ନାଥ ମନ୍ଦିରକୁ ନେଇଯାଇଥିଲେ। ଏହି ସୂର୍ଯ୍ୟ ମନ୍ଦିର ଭାରତୀୟ ମନ୍ଦିର ସ୍ଥାପତ୍ୟର କଳିଙ୍ଗନ ବିଦ୍ୟାଳୟର ଅଟେ। ସୂର୍ଯ୍ୟମନ୍ଦିରର ପାର୍ଶ୍ୱସଜ୍ଜା ପୂର୍ବ-ପଶ୍ଚିମ ଦିଗରେ ରହିଛି। ଆଭ୍ୟନ୍ତରୀଣ ଗର୍ଭଗୃହ ବା ବିମାନ ଉପରେ ଏକ ଟାୱାର କିମ୍ବା ଶିକାରା ଥିଲା, କିନ୍ତୁ ଏହା ଉନବିଂଶ ଶତାବ୍ଦୀରେ ଧ୍ୱଂସ ହୋଇଯାଇଥିଲା। ଦର୍ଶକ ହଲ୍ ବା ଜଗମୋହନ ଏବେ ବି ଛିଡ଼ା ହୋଇ ରହିଛି ଏବଂ ସେଥିରେ ଅଧିକାଂଶ ଭଗ୍ନାବଶେଷ ରହିଛି। ନୃତ୍ୟ ଗୃହ କିମ୍ବା ନାଟମନ୍ଦିରର ଛାତ ଉଡିଯାଇଛି। ଏହା ଭଗ୍ନାବଶେଷର ପୂର୍ବ ଦିଗରେ ଏକ ଉପରକୁ ଉଠିଥିବା ପ୍ଲାଟଫର୍ମରେ ରହିଛି। ୧୫୫୯ ମସିହାରେ ମୁକୁନ୍ଦ ଗଜପତି କଟକର ସିଂହାସନ ସମ୍ଭାଳିଥିଲେ। ସେ ନିଜକୁ ଆକବରଙ୍କ ମିତ୍ର ଏବଂ ବଙ୍ଗଳାର ସୁଲତାନ ସୁଲେମାନ ଖାନ କରରାନୀଙ୍କ ଶତ୍ରୁ ଭାବେ ସାବ୍ୟସ୍ତ କରିଥିଲେ। କିଛି ଲଢ଼େଇ ପରେ ଶେଷରେ ଓଡିଶାର ପରାଜୟ ଘଟିଥିଲା। ରାଜ୍ୟର ଆଭ୍ୟନ୍ତରୀଣ ଅସ୍ଥିରତା କାରଣରୁ ମଧ୍ୟ ଏହି ହ୍ରାସ ଘଟିଛି। ୧୫୬୮ ମସିହାରେ ସୁଲତାନର ସେନାପତି କାଲାପାହାଡ଼ଙ୍କ ସେନା କୋଣାର୍କ ମନ୍ଦିରକୁ କ୍ଷତିଗ୍ରସ୍ତ କରିଥିଲେ। ବିଜୟ ସମୟରେ ଅନ୍ୟ କେତେକ ମନ୍ଦିରର କ୍ଷୟକ୍ଷତି ପାଇଁ ମଧ୍ୟ କଳାପାହାଡ଼ ଦାୟୀ ବୋଲି କୁହାଯାଏ।'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indic to English Language Translation "
      ],
      "metadata": {
        "id": "P6a_6tpOUZiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "indic2en_model = Model(expdir='/content/model_files/indic-en')\n",
        "#\n",
        "INDIC = {\"Assamese\": \"as\", \"Bengali\": \"bn\", \"Gujarati\": \"gu\", \"Hindi\": \"hi\",\"Kannada\": \"kn\",\"Malayalam\": \"ml\", \"Marathi\": \"mr\", \"Odia\": \"or\",\"Punjabi\": \"pa\",\"Tamil\": \"ta\", \"Telugu\" : \"te\"}\n",
        "#\n",
        "languages = list(INDIC.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "eqH9XvkgX7al",
        "outputId": "1594d72a-17a7-4623-cb42-09860cd34501"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing vocab and bpe\n",
            "Initializing model for translation\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cedf4ef87af5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mindic2en_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/model_files/indic-en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mINDIC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Assamese\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"as\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Bengali\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"bn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Gujarati\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"gu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Hindi\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"hi\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Kannada\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"kn\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Malayalam\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"ml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Marathi\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"mr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Odia\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"or\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Punjabi\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pa\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Tamil\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"ta\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Telugu\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"te\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ea129db396d7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, expdir)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializing model for translation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;31m# initialize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         self.translator = Translator(\n\u001b[0m\u001b[1;32m    383\u001b[0m             \u001b[0;34mf\"{expdir}/final_bin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{expdir}/model/checkpoint_best.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-2-ea129db396d7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, checkpoint_path, batch_size, constrained_decoding)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_namespace_to_omegaconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_user_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteractive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_size\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fairseq/utils.py\u001b[0m in \u001b[0;36mimport_user_module\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    479\u001b[0m                     \u001b[0mmodule_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfairseq_rel_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;31m# ensure that user modules are only imported once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: /content/model_configs"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = ['एलन मस्क ने टेस्ला के 8.5 अरब डॉलर के शेयर बेचे',\n",
        " 'मैं एक पेशेवर अकादमिक और शोध लेखक हूं।',\n",
        " 'अमेरिका में नौकरी करो और जर्मनी में काम करो']"
      ],
      "metadata": {
        "id": "798YmZ1SYe4q"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate2EnglishP(text, lang):\n",
        "  return indic2en_model.translate_paragraph(text, INDIC[lang], 'en')\n",
        "def translate2EnglishB(text, lang):\n",
        "  return indic2en_model.batch_translate(text,INDIC[lang], 'en')"
      ],
      "metadata": {
        "id": "JEGjHx1QQtnZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate2EnglishB(inputs, 'Hindi')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrM8-9NUYtw1",
        "outputId": "dbd60d46-9c08-4cf5-a8db-d9be5c86f58c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 2448.51it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Elon Musk sells $8.5 billion Tesla shares',\n",
              " 'I am a professional academic and research writer.',\n",
              " 'Work in America and work in Germany']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLy33-H4ZB5o",
        "outputId": "89df7ee7-8889-4157-b9a8-fe8ba87a3244"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil"
      ],
      "metadata": {
        "id": "IaACdqpmqhuc"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Inference Script"
      ],
      "metadata": {
        "id": "ZcBqNKAbtrvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.curdir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "RO0dljfDRVOg",
        "outputId": "52aafdf8-c688-490c-9372-be2d7df2e2e8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "from fairseq import checkpoint_utils, options, tasks, utils\n",
        "from fairseq.dataclass.utils import convert_namespace_to_omegaconf\n",
        "from fairseq.token_generation_constraints import pack_constraints, unpack_constraints\n",
        "from fairseq_cli.generate import get_symbols_to_strip_from_output\n",
        "#\n",
        "from fairseq.models import register_model_architecture\n",
        "from fairseq.models.transformer import base_architecture\n",
        "#\n",
        "\n",
        "import codecs\n",
        "import os\n",
        "import json\n",
        "#\n",
        "from os import truncate\n",
        "from sacremoses import MosesPunctNormalizer\n",
        "from sacremoses import MosesTokenizer\n",
        "from sacremoses import MosesDetokenizer\n",
        "from subword_nmt.apply_bpe import BPE, read_vocabulary\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "from indicnlp.tokenize import indic_detokenize\n",
        "from indicnlp.normalize import indic_normalize\n",
        "from indicnlp.transliterate import unicode_transliterate\n",
        "from mosestokenizer import MosesSentenceSplitter\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "#\n",
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils\n",
        "#\n",
        "#Custom Interactive\n",
        "#\n",
        "Batch = namedtuple(\"Batch\", \"ids src_tokens src_lengths constraints\")\n",
        "Translation = namedtuple(\"Translation\", \"src_str hypos pos_scores alignments\")\n",
        "\n",
        "\n",
        "def make_batches(\n",
        "    lines, cfg, task, max_positions, encode_fn, constrainted_decoding=False\n",
        "):\n",
        "    def encode_fn_target(x):\n",
        "        return encode_fn(x)\n",
        "\n",
        "    if constrainted_decoding:\n",
        "        # Strip (tab-delimited) contraints, if present, from input lines,\n",
        "        # store them in batch_constraints\n",
        "        batch_constraints = [list() for _ in lines]\n",
        "        for i, line in enumerate(lines):\n",
        "            if \"\\t\" in line:\n",
        "                lines[i], *batch_constraints[i] = line.split(\"\\t\")\n",
        "\n",
        "        # Convert each List[str] to List[Tensor]\n",
        "        for i, constraint_list in enumerate(batch_constraints):\n",
        "            batch_constraints[i] = [\n",
        "                task.target_dictionary.encode_line(\n",
        "                    encode_fn_target(constraint),\n",
        "                    append_eos=False,\n",
        "                    add_if_not_exist=False,\n",
        "                )\n",
        "                for constraint in constraint_list\n",
        "            ]\n",
        "\n",
        "    if constrainted_decoding:\n",
        "        constraints_tensor = pack_constraints(batch_constraints)\n",
        "    else:\n",
        "        constraints_tensor = None\n",
        "\n",
        "    tokens, lengths = task.get_interactive_tokens_and_lengths(lines, encode_fn)\n",
        "\n",
        "    itr = task.get_batch_iterator(\n",
        "        dataset=task.build_dataset_for_inference(\n",
        "            tokens, lengths, constraints=constraints_tensor\n",
        "        ),\n",
        "        max_tokens=cfg.dataset.max_tokens,\n",
        "        max_sentences=cfg.dataset.batch_size,\n",
        "        max_positions=max_positions,\n",
        "        ignore_invalid_inputs=cfg.dataset.skip_invalid_size_inputs_valid_test,\n",
        "    ).next_epoch_itr(shuffle=False)\n",
        "    for batch in itr:\n",
        "        ids = batch[\"id\"]\n",
        "        src_tokens = batch[\"net_input\"][\"src_tokens\"]\n",
        "        src_lengths = batch[\"net_input\"][\"src_lengths\"]\n",
        "        constraints = batch.get(\"constraints\", None)\n",
        "\n",
        "        yield Batch(\n",
        "            ids=ids,\n",
        "            src_tokens=src_tokens,\n",
        "            src_lengths=src_lengths,\n",
        "            constraints=constraints,\n",
        "        )\n",
        "\n",
        "\n",
        "class Translator:\n",
        "    def __init__(\n",
        "        self, model_path,data_dir, checkpoint_path, batch_size=25, constrained_decoding=False\n",
        "    ):\n",
        "\n",
        "        self.constrained_decoding = constrained_decoding\n",
        "        self.modl_path = model_path\n",
        "        self.parser = options.get_generation_parser(interactive=True)\n",
        "        # buffer_size is currently not used but we just initialize it to batch\n",
        "        # size + 1 to avoid any assertion errors.\n",
        "        if self.constrained_decoding:\n",
        "            self.parser.set_defaults(\n",
        "                path=checkpoint_path,\n",
        "                remove_bpe=\"subword_nmt\",\n",
        "                num_workers=-1,\n",
        "                constraints=\"ordered\",\n",
        "                batch_size=batch_size,\n",
        "                buffer_size=batch_size + 1,\n",
        "            )\n",
        "        else:\n",
        "            self.parser.set_defaults(\n",
        "                path=checkpoint_path,\n",
        "                remove_bpe=\"subword_nmt\",\n",
        "                num_workers=-1,\n",
        "                batch_size=batch_size,\n",
        "                buffer_size=batch_size + 1,\n",
        "            )\n",
        "        args = options.parse_args_and_arch(self.parser, input_args=[data_dir])\n",
        "        # we are explictly setting src_lang and tgt_lang here\n",
        "        # generally the data_dir we pass contains {split}-{src_lang}-{tgt_lang}.*.idx files from\n",
        "        # which fairseq infers the src and tgt langs(if these are not passed). In deployment we dont\n",
        "        # use any idx files and only store the SRC and TGT dictionaries.\n",
        "        args.source_lang = \"SRC\"\n",
        "        args.target_lang = \"TGT\"\n",
        "        # since we are truncating sentences to max_seq_len in engine, we can set it to False here\n",
        "        args.skip_invalid_size_inputs_valid_test = False\n",
        "\n",
        "        # we have custom architechtures in this folder and we will let fairseq\n",
        "        # import this\n",
        "        model_file_path = os.path.join(model_path,\"en-indic\",\"model_configs\")\n",
        "        args.user_dir = model_file_path\n",
        "        self.cfg = convert_namespace_to_omegaconf(args)\n",
        "\n",
        "        utils.import_user_module(self.cfg.common)\n",
        "\n",
        "        if self.cfg.interactive.buffer_size < 1:\n",
        "            self.cfg.interactive.buffer_size = 1\n",
        "        if self.cfg.dataset.max_tokens is None and self.cfg.dataset.batch_size is None:\n",
        "            self.cfg.dataset.batch_size = 1\n",
        "\n",
        "        assert (\n",
        "            not self.cfg.generation.sampling\n",
        "            or self.cfg.generation.nbest == self.cfg.generation.beam\n",
        "        ), \"--sampling requires --nbest to be equal to --beam\"\n",
        "        assert (\n",
        "            not self.cfg.dataset.batch_size\n",
        "            or self.cfg.dataset.batch_size <= self.cfg.interactive.buffer_size\n",
        "        ), \"--batch-size cannot be larger than --buffer-size\"\n",
        "\n",
        "        # Fix seed for stochastic decoding\n",
        "        # if self.cfg.common.seed is not None and not self.cfg.generation.no_seed_provided:\n",
        "        #     np.random.seed(self.cfg.common.seed)\n",
        "        #     utils.set_torch_seed(self.cfg.common.seed)\n",
        "\n",
        "        # if not self.constrained_decoding:\n",
        "        #     self.use_cuda = torch.cuda.is_available() and not self.cfg.common.cpu\n",
        "        # else:\n",
        "        #     self.use_cuda = False\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available() and not self.cfg.common.cpu\n",
        "\n",
        "        # Setup task, e.g., translation\n",
        "        self.task = tasks.setup_task(self.cfg.task)\n",
        "\n",
        "        # Load ensemble\n",
        "        overrides = ast.literal_eval(self.cfg.common_eval.model_overrides)\n",
        "        self.models, self._model_args = checkpoint_utils.load_model_ensemble(\n",
        "            utils.split_paths(self.cfg.common_eval.path),\n",
        "            arg_overrides=overrides,\n",
        "            task=self.task,\n",
        "            suffix=self.cfg.checkpoint.checkpoint_suffix,\n",
        "            strict=(self.cfg.checkpoint.checkpoint_shard_count == 1),\n",
        "            num_shards=self.cfg.checkpoint.checkpoint_shard_count,\n",
        "        )\n",
        "\n",
        "        # Set dictionaries\n",
        "        self.src_dict = self.task.source_dictionary\n",
        "        self.tgt_dict = self.task.target_dictionary\n",
        "\n",
        "        # Optimize ensemble for generation\n",
        "        for model in self.models:\n",
        "            if model is None:\n",
        "                continue\n",
        "            if self.cfg.common.fp16:\n",
        "                model.half()\n",
        "            if (\n",
        "                self.use_cuda\n",
        "                and not self.cfg.distributed_training.pipeline_model_parallel\n",
        "            ):\n",
        "                model.cuda()\n",
        "            model.prepare_for_inference_(self.cfg)\n",
        "\n",
        "        # Initialize generator\n",
        "        self.generator = self.task.build_generator(self.models, self.cfg.generation)\n",
        "\n",
        "        # Handle tokenization and BPE\n",
        "        self.tokenizer = self.task.build_tokenizer(self.cfg.tokenizer)\n",
        "        self.bpe = self.task.build_bpe(self.cfg.bpe)\n",
        "\n",
        "        # Load alignment dictionary for unknown word replacement\n",
        "        # (None if no unknown word replacement, empty if no path to align dictionary)\n",
        "        self.align_dict = utils.load_align_dict(self.cfg.generation.replace_unk)\n",
        "\n",
        "        self.max_positions = utils.resolve_max_positions(\n",
        "            self.task.max_positions(), *[model.max_positions() for model in self.models]\n",
        "        )\n",
        "\n",
        "    def encode_fn(self, x):\n",
        "        if self.tokenizer is not None:\n",
        "            x = self.tokenizer.encode(x)\n",
        "        if self.bpe is not None:\n",
        "            x = self.bpe.encode(x)\n",
        "        return x\n",
        "\n",
        "    def decode_fn(self, x):\n",
        "        if self.bpe is not None:\n",
        "            x = self.bpe.decode(x)\n",
        "        if self.tokenizer is not None:\n",
        "            x = self.tokenizer.decode(x)\n",
        "        return x\n",
        "\n",
        "    def translate(self, inputs, constraints=None):\n",
        "        if self.constrained_decoding and constraints is None:\n",
        "            raise ValueError(\"Constraints cant be None in constrained decoding mode\")\n",
        "        if not self.constrained_decoding and constraints is not None:\n",
        "            raise ValueError(\"Cannot pass constraints during normal translation\")\n",
        "        if constraints:\n",
        "            constrained_decoding = True\n",
        "            modified_inputs = []\n",
        "            for _input, constraint in zip(inputs, constraints):\n",
        "                modified_inputs.append(_input + f\"\\t{constraint}\")\n",
        "            inputs = modified_inputs\n",
        "        else:\n",
        "            constrained_decoding = False\n",
        "\n",
        "        start_id = 0\n",
        "        results = []\n",
        "        final_translations = []\n",
        "        for batch in make_batches(\n",
        "            inputs,\n",
        "            self.cfg,\n",
        "            self.task,\n",
        "            self.max_positions,\n",
        "            self.encode_fn,\n",
        "            constrained_decoding,\n",
        "        ):\n",
        "            bsz = batch.src_tokens.size(0)\n",
        "            src_tokens = batch.src_tokens\n",
        "            src_lengths = batch.src_lengths\n",
        "            constraints = batch.constraints\n",
        "            if self.use_cuda:\n",
        "                src_tokens = src_tokens.cuda()\n",
        "                src_lengths = src_lengths.cuda()\n",
        "                if constraints is not None:\n",
        "                    constraints = constraints.cuda()\n",
        "\n",
        "            sample = {\n",
        "                \"net_input\": {\n",
        "                    \"src_tokens\": src_tokens,\n",
        "                    \"src_lengths\": src_lengths,\n",
        "                },\n",
        "            }\n",
        "\n",
        "            translations = self.task.inference_step(\n",
        "                self.generator, self.models, sample, constraints=constraints\n",
        "            )\n",
        "\n",
        "            list_constraints = [[] for _ in range(bsz)]\n",
        "            if constrained_decoding:\n",
        "                list_constraints = [unpack_constraints(c) for c in constraints]\n",
        "            for i, (id, hypos) in enumerate(zip(batch.ids.tolist(), translations)):\n",
        "                src_tokens_i = utils.strip_pad(src_tokens[i], self.tgt_dict.pad())\n",
        "                constraints = list_constraints[i]\n",
        "                results.append(\n",
        "                    (\n",
        "                        start_id + id,\n",
        "                        src_tokens_i,\n",
        "                        hypos,\n",
        "                        {\n",
        "                            \"constraints\": constraints,\n",
        "                        },\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        # sort output to match input order\n",
        "        for id_, src_tokens, hypos, _ in sorted(results, key=lambda x: x[0]):\n",
        "            src_str = \"\"\n",
        "            if self.src_dict is not None:\n",
        "                src_str = self.src_dict.string(\n",
        "                    src_tokens, self.cfg.common_eval.post_process\n",
        "                )\n",
        "\n",
        "            # Process top predictions\n",
        "            for hypo in hypos[: min(len(hypos), self.cfg.generation.nbest)]:\n",
        "                hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n",
        "                    hypo_tokens=hypo[\"tokens\"].int().cpu(),\n",
        "                    src_str=src_str,\n",
        "                    alignment=hypo[\"alignment\"],\n",
        "                    align_dict=self.align_dict,\n",
        "                    tgt_dict=self.tgt_dict,\n",
        "                    remove_bpe=\"subword_nmt\",\n",
        "                    extra_symbols_to_ignore=get_symbols_to_strip_from_output(\n",
        "                        self.generator\n",
        "                    ),\n",
        "                )\n",
        "                detok_hypo_str = self.decode_fn(hypo_str)\n",
        "                final_translations.append(detok_hypo_str)\n",
        "        return final_translations\n",
        "    #\n",
        "    #engine\n",
        "    INDIC = [\"as\", \"bn\", \"gu\", \"hi\", \"kn\", \"ml\", \"mr\", \"or\", \"pa\", \"ta\", \"te\"]\n",
        "\n",
        "\n",
        "def split_sentences(paragraph, language):\n",
        "    if language == \"en\":\n",
        "        with MosesSentenceSplitter(language) as splitter:\n",
        "            return splitter([paragraph])\n",
        "    elif language in INDIC:\n",
        "        return sentence_tokenize.sentence_split(paragraph, lang=language)\n",
        "\n",
        "\n",
        "def add_token(sent, tag_infos):\n",
        "    \"\"\"add special tokens specified by tag_infos to each element in list\n",
        "    tag_infos: list of tuples (tag_type,tag)\n",
        "    each tag_info results in a token of the form: __{tag_type}__{tag}__\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = []\n",
        "    for tag_type, tag in tag_infos:\n",
        "        token = \"__\" + tag_type + \"__\" + tag + \"__\"\n",
        "        tokens.append(token)\n",
        "\n",
        "    return \" \".join(tokens) + \" \" + sent\n",
        "\n",
        "\n",
        "def apply_lang_tags(sents, src_lang, tgt_lang):\n",
        "    tagged_sents = []\n",
        "    for sent in sents:\n",
        "        tagged_sent = add_token(sent.strip(), [(\"src\", src_lang), (\"tgt\", tgt_lang)])\n",
        "        tagged_sents.append(tagged_sent)\n",
        "    return tagged_sents\n",
        "\n",
        "\n",
        "def truncate_long_sentences(sents):\n",
        "\n",
        "    MAX_SEQ_LEN = 200\n",
        "    new_sents = []\n",
        "\n",
        "    for sent in sents:\n",
        "        words = sent.split()\n",
        "        num_words = len(words)\n",
        "        if num_words > MAX_SEQ_LEN:\n",
        "            print_str = \" \".join(words[:5]) + \" .... \" + \" \".join(words[-5:])\n",
        "            sent = \" \".join(words[:MAX_SEQ_LEN])\n",
        "            print(\n",
        "                f\"WARNING: Sentence {print_str} truncated to 200 tokens as it exceeds maximum length limit\"\n",
        "            )\n",
        "\n",
        "        new_sents.append(sent)\n",
        "    return new_sents\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, expdir,model_path):\n",
        "        self.expdir = expdir\n",
        "        self.model_path = model_path\n",
        "        self.en_tok = MosesTokenizer(lang=\"en\")\n",
        "        self.en_normalizer = MosesPunctNormalizer()\n",
        "        self.en_detok = MosesDetokenizer(lang=\"en\")\n",
        "        self.xliterator = unicode_transliterate.UnicodeIndicTransliterator()\n",
        "        print(\"Initializing vocab and bpe\")\n",
        "        self.vocabulary = read_vocabulary(\n",
        "            codecs.open(f\"{expdir}/vocab/vocab.SRC\", encoding=\"utf-8\"), 5\n",
        "        )\n",
        "        self.bpe = BPE(\n",
        "            codecs.open(f\"{expdir}/vocab/bpe_codes.32k.SRC\", encoding=\"utf-8\"),\n",
        "            -1,\n",
        "            \"@@\",\n",
        "            self.vocabulary,\n",
        "            None,\n",
        "        )\n",
        "\n",
        "        print(\"Initializing model for translation\")\n",
        "        # initialize the model\n",
        "        self.translator = Translator(\n",
        "            model_path,f\"{expdir}/final_bin\", f\"{expdir}/model/checkpoint_best.pt\", batch_size=100\n",
        "        )\n",
        "\n",
        "    # translate a batch of sentences from src_lang to tgt_lang\n",
        "    def batch_translate(self, batch, src_lang, tgt_lang):\n",
        "\n",
        "        assert isinstance(batch, list)\n",
        "        preprocessed_sents = self.preprocess(batch, lang=src_lang)\n",
        "        bpe_sents = self.apply_bpe(preprocessed_sents)\n",
        "        tagged_sents = apply_lang_tags(bpe_sents, src_lang, tgt_lang)\n",
        "        tagged_sents = truncate_long_sentences(tagged_sents)\n",
        "\n",
        "        translations = self.translator.translate(tagged_sents)\n",
        "        postprocessed_sents = self.postprocess(translations, tgt_lang)\n",
        "\n",
        "        return postprocessed_sents\n",
        "\n",
        "    # translate a paragraph from src_lang to tgt_lang\n",
        "    def translate_paragraph(self, paragraph, src_lang, tgt_lang):\n",
        "\n",
        "        assert isinstance(paragraph, str)\n",
        "        sents = split_sentences(paragraph, src_lang)\n",
        "\n",
        "        postprocessed_sents = self.batch_translate(sents, src_lang, tgt_lang)\n",
        "\n",
        "        translated_paragraph = \" \".join(postprocessed_sents)\n",
        "\n",
        "        return translated_paragraph\n",
        "\n",
        "    def preprocess_sent(self, sent, normalizer, lang):\n",
        "        if lang == \"en\":\n",
        "            return \" \".join(\n",
        "                self.en_tok.tokenize(\n",
        "                    self.en_normalizer.normalize(sent.strip()), escape=False\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            # line = indic_detokenize.trivial_detokenize(line.strip(), lang)\n",
        "            return unicode_transliterate.UnicodeIndicTransliterator.transliterate(\n",
        "                \" \".join(\n",
        "                    indic_tokenize.trivial_tokenize(\n",
        "                        normalizer.normalize(sent.strip()), lang\n",
        "                    )\n",
        "                ),\n",
        "                lang,\n",
        "                \"hi\",\n",
        "            ).replace(\" ् \", \"्\")\n",
        "\n",
        "    def preprocess(self, sents, lang):\n",
        "        \"\"\"\n",
        "        Normalize, tokenize and script convert(for Indic)\n",
        "        return number of sentences input file\n",
        "        \"\"\"\n",
        "\n",
        "        if lang == \"en\":\n",
        "\n",
        "            # processed_sents = Parallel(n_jobs=-1, backend=\"multiprocessing\")(\n",
        "            #     delayed(preprocess_line)(line, None, lang) for line in tqdm(sents, total=num_lines)\n",
        "            # )\n",
        "            processed_sents = [\n",
        "                self.preprocess_sent(line, None, lang) for line in tqdm(sents)\n",
        "            ]\n",
        "\n",
        "        else:\n",
        "            normfactory = indic_normalize.IndicNormalizerFactory()\n",
        "            normalizer = normfactory.get_normalizer(lang)\n",
        "\n",
        "            # processed_sents = Parallel(n_jobs=-1, backend=\"multiprocessing\")(\n",
        "            #     delayed(preprocess_line)(line, normalizer, lang) for line in tqdm(infile, total=num_lines)\n",
        "            # )\n",
        "            processed_sents = [\n",
        "                self.preprocess_sent(line, normalizer, lang) for line in tqdm(sents)\n",
        "            ]\n",
        "\n",
        "        return processed_sents\n",
        "\n",
        "    def postprocess(self, sents, lang, common_lang=\"hi\"):\n",
        "        \"\"\"\n",
        "        parse fairseq interactive output, convert script back to native Indic script (in case of Indic languages) and detokenize.\n",
        "        infname: fairseq log file\n",
        "        outfname: output file of translation (sentences not translated contain the dummy string 'DUMMY_OUTPUT'\n",
        "        input_size: expected number of output sentences\n",
        "        lang: language\n",
        "        \"\"\"\n",
        "        postprocessed_sents = []\n",
        "\n",
        "        if lang == \"en\":\n",
        "            for sent in sents:\n",
        "                # outfile.write(en_detok.detokenize(sent.split(\" \")) + \"\\n\")\n",
        "                postprocessed_sents.append(self.en_detok.detokenize(sent.split(\" \")))\n",
        "        else:\n",
        "            for sent in sents:\n",
        "                outstr = indic_detokenize.trivial_detokenize(\n",
        "                    self.xliterator.transliterate(sent, common_lang, lang), lang\n",
        "                )\n",
        "                # outfile.write(outstr + \"\\n\")\n",
        "                postprocessed_sents.append(outstr)\n",
        "        return postprocessed_sents\n",
        "\n",
        "    def apply_bpe(self, sents):\n",
        "\n",
        "        return [self.bpe.process_line(sent) for sent in sents]\n",
        "#\n",
        "# inference script execution template\n",
        "#\n",
        "def preprocess_function(text_path, content_type=None):    \n",
        "    with open(text_path,\"r\",encoding='utf-8') as f:\n",
        "        data = f.read()\n",
        "    #print(data)\n",
        "    # pass the textand the target tanguage to be translated separated by a \";\" semicolon\n",
        "    #data = text_path.read().decode(\"utf-8\")\n",
        "    text = data.split(\";\")[0]\n",
        "    target = data.split(\";\")[1].strip()\n",
        "    print(target.strip())\n",
        "    #print(data)\n",
        "    return (text,target)\n",
        "\n",
        "#\n",
        "#\n",
        "def predict_function(context, model):\n",
        "    text,target = context\n",
        "    #\n",
        "    INDIC = {\"Assamese\": \"as\", \"Bengali\": \"bn\", \"Gujarati\": \"gu\", \"Hindi\": \"hi\",\"Kannada\": \"kn\",\"Malayalam\": \"ml\", \"Marathi\": \"mr\", \"Odia\": \"or\",\"Punjabi\": \"pa\",\"Tamil\": \"ta\", \"Telugu\" : \"te\"}\n",
        "    #\n",
        "    input_batch = split_sentences(text, language=\"en\")\n",
        "    print(input_batch)\n",
        "    answer = model.batch_translate(input_batch, 'en', INDIC[target])\n",
        "    return \" \".join(answer)\n",
        "\n",
        "#\n",
        "def model_load_function(model_path):\n",
        "    model_file_path = os.path.join(model_path, \"en-indic\")\n",
        "    indic2en_model = Model(model_path = model_path,expdir=model_file_path)\n",
        "    return indic2en_model\n",
        "\n",
        "\n",
        "#\n",
        "def postprocess_function(predictions, content_type=None):\n",
        "    return json.dumps({\"response\":predictions},ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "## Test the script\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    txt_path = \"/content/input.txt\"\n",
        "    data = preprocess_function(txt_path)\n",
        "    model_path = \"/content/drive/MyDrive/indic/model_files\"\n",
        "    path = model_load_function(model_path)\n",
        "    predictions = predict_function(data,path)\n",
        "    out = postprocess_function(predictions)\n",
        "    print(out)\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08XC2wXmqlSy",
        "outputId": "7cf2b192-602b-40c5-b35c-49ec1e82a59c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Odia\n",
            "Initializing vocab and bpe\n",
            "Initializing model for translation\n",
            "['The Sun Temple was built in the 13th century and designed as a gigantic chariot of the Sun God, Surya, with twelve pairs of ornamented wheels pulled by seven horses.', 'Some of the wheels are 3 metres wide.', 'Only six of the seven horse still stand today.The temple fell into disuse after an envoy of Jahangir desecrated the temple in the early 17th century.', 'According to folklore, there was a diamond in the centre of the idol which reflected the sun rays that passed.', 'In 1627, the then Raja of Khurda took the Sun idol from Konark to the Jagannath temple in Puri.', 'The Sun temple belongs to the Kalingan school of Indian temple architecture.', 'The alignment of the Sun Temple is along the east–west direction.', 'The inner sanctum or vimana used to be surmounted by a tower or shikara but it was razed in the 19th century.', 'The audience hall or jagamohana still stands and comprises majority of the ruins.', 'The roof of the dance hall or natmandir has fallen off.', 'It stands at the eastern end of the ruins on a raised platform.', 'In 1559, Mukunda Gajapati came to throne in Cuttack.', 'He aligned himself as an ally of Akbar and an enemy of the Sultan of Bengal, Sulaiman Khan Karrani.', 'After a few battles, Odisha finally fell.', 'The fall was also aided by the internal turmoil of the state.', 'In 1568, the Konark temple was damaged by the army of Kalapahad, a general of the Sultan.Kalapahad is also said to be responsible for damages to several other temples during the conquest.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 4658.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"response\": \"ତ୍ରୟୋଦଶ ଶତାବ୍ଦୀରେ ନିର୍ମିତ ଏହି ସୂର୍ଯ୍ୟମନ୍ଦିର ସୂର୍ଯ୍ୟଦେବଙ୍କ ଏକ ବିଶାଳ ରଥ ଭାବେ ପରିଗଣିତ ହୋଇଥିଲା। କେତେକ ଚକା 3 ମିଟର ଚଉଡ଼ା। ସପ୍ତଦଶ ଶତାବ୍ଦୀର ପ୍ରାରମ୍ଭରେ ଜାହାଙ୍ଗୀରଙ୍କ ଦୂତ ମନ୍ଦିରକୁ ଅପବିତ୍ର କରିବା ପରେ ଏହି ମନ୍ଦିର ନଷ୍ଟ ହୋଇଯାଇଥିଲା। ଲୋକକଥା ଅନୁସାରେ ମୂର୍ତ୍ତିର ମଧ୍ୟଭାଗରେ ଗୋଟିଏ ହୀରା ଥିଲା ଯାହା ସୂର୍ଯ୍ୟକିରଣ ପ୍ରତିଫଳିତ କରୁଥିଲା। 1627 ମସିହାରେ ତତ୍କାଳୀନ ଖୋର୍ଦ୍ଧା ରାଜା ସୂର୍ଯ୍ୟମୂର୍ତ୍ତିକୁ କୋଣାର୍କରୁ ପୁରୀର ଜଗନ୍ନାଥ ମନ୍ଦିରକୁ ନେଇଯାଇଥିଲେ। ଏହି ସୂର୍ଯ୍ୟ ମନ୍ଦିର ଭାରତୀୟ ମନ୍ଦିର ସ୍ଥାପତ୍ୟର କଳିଙ୍ଗନ ବିଦ୍ୟାଳୟର ଅଟେ। ସୂର୍ଯ୍ୟମନ୍ଦିରର ପାର୍ଶ୍ୱସଜ୍ଜା ପୂର୍ବ-ପଶ୍ଚିମ ଦିଗରେ ରହିଛି। ଆଭ୍ୟନ୍ତରୀଣ ଗର୍ଭଗୃହ ବା ବିମାନ ଉପରେ ଏକ ଟାୱାର କିମ୍ବା ଶିକାରା ଥିଲା, କିନ୍ତୁ ଏହା ଉନବିଂଶ ଶତାବ୍ଦୀରେ ଧ୍ୱଂସ ହୋଇଯାଇଥିଲା। ଦର୍ଶକ ହଲ୍ ବା ଜଗମୋହନ ଏବେ ବି ଛିଡ଼ା ହୋଇ ରହିଛି ଏବଂ ସେଥିରେ ଅଧିକାଂଶ ଭଗ୍ନାବଶେଷ ରହିଛି। ନୃତ୍ୟ ଗୃହ କିମ୍ବା ନାଟମନ୍ଦିରର ଛାତ ଉଡିଯାଇଛି। ଏହା ଭଗ୍ନାବଶେଷର ପୂର୍ବ ଦିଗରେ ଏକ ଉପରକୁ ଉଠିଥିବା ପ୍ଲାଟଫର୍ମରେ ରହିଛି। ୧୫୫୯ ମସିହାରେ ମୁକୁନ୍ଦ ଗଜପତି କଟକର ସିଂହାସନ ସମ୍ଭାଳିଥିଲେ। ସେ ନିଜକୁ ଆକବରଙ୍କ ମିତ୍ର ଏବଂ ବଙ୍ଗଳାର ସୁଲତାନ ସୁଲେମାନ ଖାନ କରରାନୀଙ୍କ ଶତ୍ରୁ ଭାବେ ସାବ୍ୟସ୍ତ କରିଥିଲେ। କିଛି ଲଢ଼େଇ ପରେ ଶେଷରେ ଓଡିଶାର ପରାଜୟ ଘଟିଥିଲା। ରାଜ୍ୟର ଆଭ୍ୟନ୍ତରୀଣ ଅସ୍ଥିରତା କାରଣରୁ ମଧ୍ୟ ଏହି ହ୍ରାସ ଘଟିଛି। ୧୫୬୮ ମସିହାରେ ସୁଲତାନର ସେନାପତି କାଲାପାହାଡ଼ଙ୍କ ସେନା କୋଣାର୍କ ମନ୍ଦିରକୁ କ୍ଷତିଗ୍ରସ୍ତ କରିଥିଲେ। ବିଜୟ ସମୟରେ ଅନ୍ୟ କେତେକ ମନ୍ଦିରର କ୍ଷୟକ୍ଷତି ପାଇଁ ମଧ୍ୟ କଳାପାହାଡ଼ ଦାୟୀ ବୋଲି କୁହାଯାଏ।\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/input2.txt\n",
        "ତ୍ରୟୋଦଶ ଶତାବ୍ଦୀରେ ନିର୍ମିତ ଏହି ସୂର୍ଯ୍ୟମନ୍ଦିର ସୂର୍ଯ୍ୟଦେବଙ୍କ ଏକ ବିଶାଳ ରଥ ଭାବେ ପରିଗଣିତ ହୋଇଥିଲା। କେତେକ ଚକା 3 ମିଟର ଚଉଡ଼ା। ସପ୍ତଦଶ ଶତାବ୍ଦୀର ପ୍ରାରମ୍ଭରେ ଜାହାଙ୍ଗୀରଙ୍କ ଦୂତ ମନ୍ଦିରକୁ ଅପବିତ୍ର କରିବା ପରେ ଏହି ମନ୍ଦିର ନଷ୍ଟ ହୋଇଯାଇଥିଲା। ଲୋକକଥା ଅନୁସାରେ ମୂର୍ତ୍ତିର ମଧ୍ୟଭାଗରେ ଗୋଟିଏ ହୀରା ଥିଲା ଯାହା ସୂର୍ଯ୍ୟକିରଣ ପ୍ରତିଫଳିତ କରୁଥିଲା। 1627 ମସିହାରେ ତତ୍କାଳୀନ ଖୋର୍ଦ୍ଧା ରାଜା ସୂର୍ଯ୍ୟମୂର୍ତ୍ତିକୁ କୋଣାର୍କରୁ ପୁରୀର ଜଗନ୍ନାଥ ମନ୍ଦିରକୁ ନେଇଯାଇଥିଲେ। ଏହି ସୂର୍ଯ୍ୟ ମନ୍ଦିର ଭାରତୀୟ ମନ୍ଦିର ସ୍ଥାପତ୍ୟର କଳିଙ୍ଗନ ବିଦ୍ୟାଳୟର ଅଟେ। ସୂର୍ଯ୍ୟମନ୍ଦିରର ପାର୍ଶ୍ୱସଜ୍ଜା ପୂର୍ବ-ପଶ୍ଚିମ ଦିଗରେ ରହିଛି। ଆଭ୍ୟନ୍ତରୀଣ ଗର୍ଭଗୃହ ବା ବିମାନ ଉପରେ ଏକ ଟାୱାର କିମ୍ବା ଶିକାରା ଥିଲା, କିନ୍ତୁ ଏହା ଉନବିଂଶ ଶତାବ୍ଦୀରେ ଧ୍ୱଂସ ହୋଇଯାଇଥିଲା। ଦର୍ଶକ ହଲ୍ ବା ଜଗମୋହନ ଏବେ ବି ଛିଡ଼ା ହୋଇ ରହିଛି ଏବଂ ସେଥିରେ ଅଧିକାଂଶ ଭଗ୍ନାବଶେଷ ରହିଛି। ନୃତ୍ୟ ଗୃହ କିମ୍ବା ନାଟମନ୍ଦିରର ଛାତ ଉଡିଯାଇଛି। ଏହା ଭଗ୍ନାବଶେଷର ପୂର୍ବ ଦିଗରେ ଏକ ଉପରକୁ ଉଠିଥିବା ପ୍ଲାଟଫର୍ମରେ ରହିଛି। ୧୫୫୯ ମସିହାରେ ମୁକୁନ୍ଦ ଗଜପତି କଟକର ସିଂହାସନ ସମ୍ଭାଳିଥିଲେ। ସେ ନିଜକୁ ଆକବରଙ୍କ ମିତ୍ର ଏବଂ ବଙ୍ଗଳାର ସୁଲତାନ ସୁଲେମାନ ଖାନ କରରାନୀଙ୍କ ଶତ୍ରୁ ଭାବେ ସାବ୍ୟସ୍ତ କରିଥିଲେ। କିଛି ଲଢ଼େଇ ପରେ ଶେଷରେ ଓଡିଶାର ପରାଜୟ ଘଟିଥିଲା। ରାଜ୍ୟର ଆଭ୍ୟନ୍ତରୀଣ ଅସ୍ଥିରତା କାରଣରୁ ମଧ୍ୟ ଏହି ହ୍ରାସ ଘଟିଛି। ୧୫୬୮ ମସିହାରେ ସୁଲତାନର ସେନାପତି କାଲାପାହାଡ଼ଙ୍କ ସେନା କୋଣାର୍କ ମନ୍ଦିରକୁ କ୍ଷତିଗ୍ରସ୍ତ କରିଥିଲେ। ବିଜୟ ସମୟରେ ଅନ୍ୟ କେତେକ ମନ୍ଦିରର କ୍ଷୟକ୍ଷତି ପାଇଁ ମଧ୍ୟ କଳାପାହାଡ଼ ଦାୟୀ ବୋଲି କୁହାଯାଏ।;Odia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKSdE4TFCbJW",
        "outputId": "31426198-d827-41d7-8fee-df692e18a623"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/input2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference script indic 2 English"
      ],
      "metadata": {
        "id": "Bbv9LJl48jGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "from fairseq import checkpoint_utils, options, tasks, utils\n",
        "from fairseq.dataclass.utils import convert_namespace_to_omegaconf\n",
        "from fairseq.token_generation_constraints import pack_constraints, unpack_constraints\n",
        "from fairseq_cli.generate import get_symbols_to_strip_from_output\n",
        "\n",
        "import codecs\n",
        "import os\n",
        "import json\n",
        "#\n",
        "from os import truncate\n",
        "from sacremoses import MosesPunctNormalizer\n",
        "from sacremoses import MosesTokenizer\n",
        "from sacremoses import MosesDetokenizer\n",
        "from subword_nmt.apply_bpe import BPE, read_vocabulary\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "from indicnlp.tokenize import indic_detokenize\n",
        "from indicnlp.normalize import indic_normalize\n",
        "from indicnlp.transliterate import unicode_transliterate\n",
        "from mosestokenizer import MosesSentenceSplitter\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "#\n",
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils\n",
        "#\n",
        "#Custom Interactive\n",
        "#\n",
        "Batch = namedtuple(\"Batch\", \"ids src_tokens src_lengths constraints\")\n",
        "Translation = namedtuple(\"Translation\", \"src_str hypos pos_scores alignments\")\n",
        "\n",
        "\n",
        "def make_batches(\n",
        "    lines, cfg, task, max_positions, encode_fn, constrainted_decoding=False\n",
        "):\n",
        "    def encode_fn_target(x):\n",
        "        return encode_fn(x)\n",
        "\n",
        "    if constrainted_decoding:\n",
        "        # Strip (tab-delimited) contraints, if present, from input lines,\n",
        "        # store them in batch_constraints\n",
        "        batch_constraints = [list() for _ in lines]\n",
        "        for i, line in enumerate(lines):\n",
        "            if \"\\t\" in line:\n",
        "                lines[i], *batch_constraints[i] = line.split(\"\\t\")\n",
        "\n",
        "        # Convert each List[str] to List[Tensor]\n",
        "        for i, constraint_list in enumerate(batch_constraints):\n",
        "            batch_constraints[i] = [\n",
        "                task.target_dictionary.encode_line(\n",
        "                    encode_fn_target(constraint),\n",
        "                    append_eos=False,\n",
        "                    add_if_not_exist=False,\n",
        "                )\n",
        "                for constraint in constraint_list\n",
        "            ]\n",
        "\n",
        "    if constrainted_decoding:\n",
        "        constraints_tensor = pack_constraints(batch_constraints)\n",
        "    else:\n",
        "        constraints_tensor = None\n",
        "\n",
        "    tokens, lengths = task.get_interactive_tokens_and_lengths(lines, encode_fn)\n",
        "\n",
        "    itr = task.get_batch_iterator(\n",
        "        dataset=task.build_dataset_for_inference(\n",
        "            tokens, lengths, constraints=constraints_tensor\n",
        "        ),\n",
        "        max_tokens=cfg.dataset.max_tokens,\n",
        "        max_sentences=cfg.dataset.batch_size,\n",
        "        max_positions=max_positions,\n",
        "        ignore_invalid_inputs=cfg.dataset.skip_invalid_size_inputs_valid_test,\n",
        "    ).next_epoch_itr(shuffle=False)\n",
        "    for batch in itr:\n",
        "        ids = batch[\"id\"]\n",
        "        src_tokens = batch[\"net_input\"][\"src_tokens\"]\n",
        "        src_lengths = batch[\"net_input\"][\"src_lengths\"]\n",
        "        constraints = batch.get(\"constraints\", None)\n",
        "\n",
        "        yield Batch(\n",
        "            ids=ids,\n",
        "            src_tokens=src_tokens,\n",
        "            src_lengths=src_lengths,\n",
        "            constraints=constraints,\n",
        "        )\n",
        "\n",
        "\n",
        "class Translator:\n",
        "    def __init__(\n",
        "        self, data_dir, checkpoint_path, batch_size=25, constrained_decoding=False\n",
        "    ):\n",
        "\n",
        "        self.constrained_decoding = constrained_decoding\n",
        "        self.parser = options.get_generation_parser(interactive=True)\n",
        "        # buffer_size is currently not used but we just initialize it to batch\n",
        "        # size + 1 to avoid any assertion errors.\n",
        "        if self.constrained_decoding:\n",
        "            self.parser.set_defaults(\n",
        "                path=checkpoint_path,\n",
        "                remove_bpe=\"subword_nmt\",\n",
        "                num_workers=-1,\n",
        "                constraints=\"ordered\",\n",
        "                batch_size=batch_size,\n",
        "                buffer_size=batch_size + 1,\n",
        "            )\n",
        "        else:\n",
        "            self.parser.set_defaults(\n",
        "                path=checkpoint_path,\n",
        "                remove_bpe=\"subword_nmt\",\n",
        "                num_workers=-1,\n",
        "                batch_size=batch_size,\n",
        "                buffer_size=batch_size + 1,\n",
        "            )\n",
        "        args = options.parse_args_and_arch(self.parser, input_args=[data_dir])\n",
        "        # we are explictly setting src_lang and tgt_lang here\n",
        "        # generally the data_dir we pass contains {split}-{src_lang}-{tgt_lang}.*.idx files from\n",
        "        # which fairseq infers the src and tgt langs(if these are not passed). In deployment we dont\n",
        "        # use any idx files and only store the SRC and TGT dictionaries.\n",
        "        args.source_lang = \"SRC\"\n",
        "        args.target_lang = \"TGT\"\n",
        "        # since we are truncating sentences to max_seq_len in engine, we can set it to False here\n",
        "        args.skip_invalid_size_inputs_valid_test = False\n",
        "\n",
        "        # we have custom architechtures in this folder and we will let fairseq\n",
        "        # import this\n",
        "        args.user_dir = \"model_configs\"\n",
        "        self.cfg = convert_namespace_to_omegaconf(args)\n",
        "\n",
        "        utils.import_user_module(self.cfg.common)\n",
        "\n",
        "        if self.cfg.interactive.buffer_size < 1:\n",
        "            self.cfg.interactive.buffer_size = 1\n",
        "        if self.cfg.dataset.max_tokens is None and self.cfg.dataset.batch_size is None:\n",
        "            self.cfg.dataset.batch_size = 1\n",
        "\n",
        "        assert (\n",
        "            not self.cfg.generation.sampling\n",
        "            or self.cfg.generation.nbest == self.cfg.generation.beam\n",
        "        ), \"--sampling requires --nbest to be equal to --beam\"\n",
        "        assert (\n",
        "            not self.cfg.dataset.batch_size\n",
        "            or self.cfg.dataset.batch_size <= self.cfg.interactive.buffer_size\n",
        "        ), \"--batch-size cannot be larger than --buffer-size\"\n",
        "\n",
        "        # Fix seed for stochastic decoding\n",
        "        # if self.cfg.common.seed is not None and not self.cfg.generation.no_seed_provided:\n",
        "        #     np.random.seed(self.cfg.common.seed)\n",
        "        #     utils.set_torch_seed(self.cfg.common.seed)\n",
        "\n",
        "        # if not self.constrained_decoding:\n",
        "        #     self.use_cuda = torch.cuda.is_available() and not self.cfg.common.cpu\n",
        "        # else:\n",
        "        #     self.use_cuda = False\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available() and not self.cfg.common.cpu\n",
        "\n",
        "        # Setup task, e.g., translation\n",
        "        self.task = tasks.setup_task(self.cfg.task)\n",
        "\n",
        "        # Load ensemble\n",
        "        overrides = ast.literal_eval(self.cfg.common_eval.model_overrides)\n",
        "        self.models, self._model_args = checkpoint_utils.load_model_ensemble(\n",
        "            utils.split_paths(self.cfg.common_eval.path),\n",
        "            arg_overrides=overrides,\n",
        "            task=self.task,\n",
        "            suffix=self.cfg.checkpoint.checkpoint_suffix,\n",
        "            strict=(self.cfg.checkpoint.checkpoint_shard_count == 1),\n",
        "            num_shards=self.cfg.checkpoint.checkpoint_shard_count,\n",
        "        )\n",
        "\n",
        "        # Set dictionaries\n",
        "        self.src_dict = self.task.source_dictionary\n",
        "        self.tgt_dict = self.task.target_dictionary\n",
        "\n",
        "        # Optimize ensemble for generation\n",
        "        for model in self.models:\n",
        "            if model is None:\n",
        "                continue\n",
        "            if self.cfg.common.fp16:\n",
        "                model.half()\n",
        "            if (\n",
        "                self.use_cuda\n",
        "                and not self.cfg.distributed_training.pipeline_model_parallel\n",
        "            ):\n",
        "                model.cuda()\n",
        "            model.prepare_for_inference_(self.cfg)\n",
        "\n",
        "        # Initialize generator\n",
        "        self.generator = self.task.build_generator(self.models, self.cfg.generation)\n",
        "\n",
        "        # Handle tokenization and BPE\n",
        "        self.tokenizer = self.task.build_tokenizer(self.cfg.tokenizer)\n",
        "        self.bpe = self.task.build_bpe(self.cfg.bpe)\n",
        "\n",
        "        # Load alignment dictionary for unknown word replacement\n",
        "        # (None if no unknown word replacement, empty if no path to align dictionary)\n",
        "        self.align_dict = utils.load_align_dict(self.cfg.generation.replace_unk)\n",
        "\n",
        "        self.max_positions = utils.resolve_max_positions(\n",
        "            self.task.max_positions(), *[model.max_positions() for model in self.models]\n",
        "        )\n",
        "\n",
        "    def encode_fn(self, x):\n",
        "        if self.tokenizer is not None:\n",
        "            x = self.tokenizer.encode(x)\n",
        "        if self.bpe is not None:\n",
        "            x = self.bpe.encode(x)\n",
        "        return x\n",
        "\n",
        "    def decode_fn(self, x):\n",
        "        if self.bpe is not None:\n",
        "            x = self.bpe.decode(x)\n",
        "        if self.tokenizer is not None:\n",
        "            x = self.tokenizer.decode(x)\n",
        "        return x\n",
        "\n",
        "    def translate(self, inputs, constraints=None):\n",
        "        if self.constrained_decoding and constraints is None:\n",
        "            raise ValueError(\"Constraints cant be None in constrained decoding mode\")\n",
        "        if not self.constrained_decoding and constraints is not None:\n",
        "            raise ValueError(\"Cannot pass constraints during normal translation\")\n",
        "        if constraints:\n",
        "            constrained_decoding = True\n",
        "            modified_inputs = []\n",
        "            for _input, constraint in zip(inputs, constraints):\n",
        "                modified_inputs.append(_input + f\"\\t{constraint}\")\n",
        "            inputs = modified_inputs\n",
        "        else:\n",
        "            constrained_decoding = False\n",
        "\n",
        "        start_id = 0\n",
        "        results = []\n",
        "        final_translations = []\n",
        "        for batch in make_batches(\n",
        "            inputs,\n",
        "            self.cfg,\n",
        "            self.task,\n",
        "            self.max_positions,\n",
        "            self.encode_fn,\n",
        "            constrained_decoding,\n",
        "        ):\n",
        "            bsz = batch.src_tokens.size(0)\n",
        "            src_tokens = batch.src_tokens\n",
        "            src_lengths = batch.src_lengths\n",
        "            constraints = batch.constraints\n",
        "            if self.use_cuda:\n",
        "                src_tokens = src_tokens.cuda()\n",
        "                src_lengths = src_lengths.cuda()\n",
        "                if constraints is not None:\n",
        "                    constraints = constraints.cuda()\n",
        "\n",
        "            sample = {\n",
        "                \"net_input\": {\n",
        "                    \"src_tokens\": src_tokens,\n",
        "                    \"src_lengths\": src_lengths,\n",
        "                },\n",
        "            }\n",
        "\n",
        "            translations = self.task.inference_step(\n",
        "                self.generator, self.models, sample, constraints=constraints\n",
        "            )\n",
        "\n",
        "            list_constraints = [[] for _ in range(bsz)]\n",
        "            if constrained_decoding:\n",
        "                list_constraints = [unpack_constraints(c) for c in constraints]\n",
        "            for i, (id, hypos) in enumerate(zip(batch.ids.tolist(), translations)):\n",
        "                src_tokens_i = utils.strip_pad(src_tokens[i], self.tgt_dict.pad())\n",
        "                constraints = list_constraints[i]\n",
        "                results.append(\n",
        "                    (\n",
        "                        start_id + id,\n",
        "                        src_tokens_i,\n",
        "                        hypos,\n",
        "                        {\n",
        "                            \"constraints\": constraints,\n",
        "                        },\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        # sort output to match input order\n",
        "        for id_, src_tokens, hypos, _ in sorted(results, key=lambda x: x[0]):\n",
        "            src_str = \"\"\n",
        "            if self.src_dict is not None:\n",
        "                src_str = self.src_dict.string(\n",
        "                    src_tokens, self.cfg.common_eval.post_process\n",
        "                )\n",
        "\n",
        "            # Process top predictions\n",
        "            for hypo in hypos[: min(len(hypos), self.cfg.generation.nbest)]:\n",
        "                hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n",
        "                    hypo_tokens=hypo[\"tokens\"].int().cpu(),\n",
        "                    src_str=src_str,\n",
        "                    alignment=hypo[\"alignment\"],\n",
        "                    align_dict=self.align_dict,\n",
        "                    tgt_dict=self.tgt_dict,\n",
        "                    remove_bpe=\"subword_nmt\",\n",
        "                    extra_symbols_to_ignore=get_symbols_to_strip_from_output(\n",
        "                        self.generator\n",
        "                    ),\n",
        "                )\n",
        "                detok_hypo_str = self.decode_fn(hypo_str)\n",
        "                final_translations.append(detok_hypo_str)\n",
        "        return final_translations\n",
        "    #\n",
        "    #engine\n",
        "    INDIC = [\"as\", \"bn\", \"gu\", \"hi\", \"kn\", \"ml\", \"mr\", \"or\", \"pa\", \"ta\", \"te\"]\n",
        "\n",
        "\n",
        "def split_sentences(paragraph, language):\n",
        "    if language == \"en\":\n",
        "        with MosesSentenceSplitter(language) as splitter:\n",
        "            return splitter([paragraph])\n",
        "    elif language in INDIC:\n",
        "        return sentence_tokenize.sentence_split(paragraph, lang=language)\n",
        "\n",
        "\n",
        "def add_token(sent, tag_infos):\n",
        "    \"\"\"add special tokens specified by tag_infos to each element in list\n",
        "    tag_infos: list of tuples (tag_type,tag)\n",
        "    each tag_info results in a token of the form: __{tag_type}__{tag}__\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = []\n",
        "    for tag_type, tag in tag_infos:\n",
        "        token = \"__\" + tag_type + \"__\" + tag + \"__\"\n",
        "        tokens.append(token)\n",
        "\n",
        "    return \" \".join(tokens) + \" \" + sent\n",
        "\n",
        "\n",
        "def apply_lang_tags(sents, src_lang, tgt_lang):\n",
        "    tagged_sents = []\n",
        "    for sent in sents:\n",
        "        tagged_sent = add_token(sent.strip(), [(\"src\", src_lang), (\"tgt\", tgt_lang)])\n",
        "        tagged_sents.append(tagged_sent)\n",
        "    return tagged_sents\n",
        "\n",
        "\n",
        "def truncate_long_sentences(sents):\n",
        "\n",
        "    MAX_SEQ_LEN = 200\n",
        "    new_sents = []\n",
        "\n",
        "    for sent in sents:\n",
        "        words = sent.split()\n",
        "        num_words = len(words)\n",
        "        if num_words > MAX_SEQ_LEN:\n",
        "            print_str = \" \".join(words[:5]) + \" .... \" + \" \".join(words[-5:])\n",
        "            sent = \" \".join(words[:MAX_SEQ_LEN])\n",
        "            print(\n",
        "                f\"WARNING: Sentence {print_str} truncated to 200 tokens as it exceeds maximum length limit\"\n",
        "            )\n",
        "\n",
        "        new_sents.append(sent)\n",
        "    return new_sents\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, expdir):\n",
        "        self.expdir = expdir\n",
        "        self.en_tok = MosesTokenizer(lang=\"en\")\n",
        "        self.en_normalizer = MosesPunctNormalizer()\n",
        "        self.en_detok = MosesDetokenizer(lang=\"en\")\n",
        "        self.xliterator = unicode_transliterate.UnicodeIndicTransliterator()\n",
        "        print(\"Initializing vocab and bpe\")\n",
        "        self.vocabulary = read_vocabulary(\n",
        "            codecs.open(f\"{expdir}/vocab/vocab.SRC\", encoding=\"utf-8\"), 5\n",
        "        )\n",
        "        self.bpe = BPE(\n",
        "            codecs.open(f\"{expdir}/vocab/bpe_codes.32k.SRC\", encoding=\"utf-8\"),\n",
        "            -1,\n",
        "            \"@@\",\n",
        "            self.vocabulary,\n",
        "            None,\n",
        "        )\n",
        "\n",
        "        print(\"Initializing model for translation\")\n",
        "        # initialize the model\n",
        "        self.translator = Translator(\n",
        "            f\"{expdir}/final_bin\", f\"{expdir}/model/checkpoint_best.pt\", batch_size=100\n",
        "        )\n",
        "\n",
        "    # translate a batch of sentences from src_lang to tgt_lang\n",
        "    def batch_translate(self, batch, src_lang, tgt_lang):\n",
        "\n",
        "        assert isinstance(batch, list)\n",
        "        preprocessed_sents = self.preprocess(batch, lang=src_lang)\n",
        "        bpe_sents = self.apply_bpe(preprocessed_sents)\n",
        "        tagged_sents = apply_lang_tags(bpe_sents, src_lang, tgt_lang)\n",
        "        tagged_sents = truncate_long_sentences(tagged_sents)\n",
        "\n",
        "        translations = self.translator.translate(tagged_sents)\n",
        "        postprocessed_sents = self.postprocess(translations, tgt_lang)\n",
        "\n",
        "        return postprocessed_sents\n",
        "\n",
        "    # translate a paragraph from src_lang to tgt_lang\n",
        "    def translate_paragraph(self, paragraph, src_lang, tgt_lang):\n",
        "\n",
        "        assert isinstance(paragraph, str)\n",
        "        sents = split_sentences(paragraph, src_lang)\n",
        "\n",
        "        postprocessed_sents = self.batch_translate(sents, src_lang, tgt_lang)\n",
        "\n",
        "        translated_paragraph = \" \".join(postprocessed_sents)\n",
        "\n",
        "        return translated_paragraph\n",
        "\n",
        "    def preprocess_sent(self, sent, normalizer, lang):\n",
        "        if lang == \"en\":\n",
        "            return \" \".join(\n",
        "                self.en_tok.tokenize(\n",
        "                    self.en_normalizer.normalize(sent.strip()), escape=False\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            # line = indic_detokenize.trivial_detokenize(line.strip(), lang)\n",
        "            return unicode_transliterate.UnicodeIndicTransliterator.transliterate(\n",
        "                \" \".join(\n",
        "                    indic_tokenize.trivial_tokenize(\n",
        "                        normalizer.normalize(sent.strip()), lang\n",
        "                    )\n",
        "                ),\n",
        "                lang,\n",
        "                \"hi\",\n",
        "            ).replace(\" ् \", \"्\")\n",
        "\n",
        "    def preprocess(self, sents, lang):\n",
        "        \"\"\"\n",
        "        Normalize, tokenize and script convert(for Indic)\n",
        "        return number of sentences input file\n",
        "        \"\"\"\n",
        "\n",
        "        if lang == \"en\":\n",
        "\n",
        "            # processed_sents = Parallel(n_jobs=-1, backend=\"multiprocessing\")(\n",
        "            #     delayed(preprocess_line)(line, None, lang) for line in tqdm(sents, total=num_lines)\n",
        "            # )\n",
        "            processed_sents = [\n",
        "                self.preprocess_sent(line, None, lang) for line in tqdm(sents)\n",
        "            ]\n",
        "\n",
        "        else:\n",
        "            normfactory = indic_normalize.IndicNormalizerFactory()\n",
        "            normalizer = normfactory.get_normalizer(lang)\n",
        "\n",
        "            # processed_sents = Parallel(n_jobs=-1, backend=\"multiprocessing\")(\n",
        "            #     delayed(preprocess_line)(line, normalizer, lang) for line in tqdm(infile, total=num_lines)\n",
        "            # )\n",
        "            processed_sents = [\n",
        "                self.preprocess_sent(line, normalizer, lang) for line in tqdm(sents)\n",
        "            ]\n",
        "\n",
        "        return processed_sents\n",
        "\n",
        "    def postprocess(self, sents, lang, common_lang=\"hi\"):\n",
        "        \"\"\"\n",
        "        parse fairseq interactive output, convert script back to native Indic script (in case of Indic languages) and detokenize.\n",
        "        infname: fairseq log file\n",
        "        outfname: output file of translation (sentences not translated contain the dummy string 'DUMMY_OUTPUT'\n",
        "        input_size: expected number of output sentences\n",
        "        lang: language\n",
        "        \"\"\"\n",
        "        postprocessed_sents = []\n",
        "\n",
        "        if lang == \"en\":\n",
        "            for sent in sents:\n",
        "                # outfile.write(en_detok.detokenize(sent.split(\" \")) + \"\\n\")\n",
        "                postprocessed_sents.append(self.en_detok.detokenize(sent.split(\" \")))\n",
        "        else:\n",
        "            for sent in sents:\n",
        "                outstr = indic_detokenize.trivial_detokenize(\n",
        "                    self.xliterator.transliterate(sent, common_lang, lang), lang\n",
        "                )\n",
        "                # outfile.write(outstr + \"\\n\")\n",
        "                postprocessed_sents.append(outstr)\n",
        "        return postprocessed_sents\n",
        "\n",
        "    def apply_bpe(self, sents):\n",
        "\n",
        "        return [self.bpe.process_line(sent) for sent in sents]\n",
        "#\n",
        "# inference script execution template\n",
        "#\n",
        "def preprocess_function(text_path, content_type=None):    \n",
        "    with open(text_path,\"r\",encoding='utf-8') as f:\n",
        "        data = f.read()\n",
        "    print(data)\n",
        "    # pass the textand the target tanguage to be translated separated by a \";\" semicolon\n",
        "    #data = text_path.read().decode(\"utf-8\")\n",
        "    text = data.split(\";\")[0]\n",
        "    target = data.split(\";\")[1].strip()\n",
        "    print(target.strip())\n",
        "    print(data)\n",
        "    return (text,target)\n",
        "\n",
        "#\n",
        "#\n",
        "def predict_function(context, model):\n",
        "    text,target = context\n",
        "    INDIC = {\"Assamese\": \"as\", \"Bengali\": \"bn\", \"Gujarati\": \"gu\", \"Hindi\": \"hi\",\"Kannada\": \"kn\",\"Malayalam\": \"ml\", \"Marathi\": \"mr\", \"Odia\": \"or\",\"Punjabi\": \"pa\",\"Tamil\": \"ta\", \"Telugu\" : \"te\"}\n",
        "    #\n",
        "    input_batch = split_sentences(text, language=INDIC[target])\n",
        "    #\n",
        "    print(input_batch)\n",
        "    answer = model.batch_translate(input_batch, INDIC[target],'en')\n",
        "    return \" \".join(answer)\n",
        "\n",
        "#\n",
        "def model_load_function(model_path):\n",
        "    model_file_path = os.path.join(model_path, \"indic-en\")\n",
        "    indic2en_model = Model(expdir=model_file_path)\n",
        "    return indic2en_model\n",
        "\n",
        "\n",
        "#\n",
        "def postprocess_function(predictions, content_type=None):\n",
        "    return json.dumps({\"response\":predictions},ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "## Test the script\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    txt_path = \"/content/input2.txt\"\n",
        "    data = preprocess_function(txt_path)\n",
        "    model_path = \"/content/model_files\"\n",
        "    path = model_load_function(model_path)\n",
        "    predictions = predict_function(data,path)\n",
        "    out = postprocess_function(predictions)\n",
        "    print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "1hjgGH_mCKst",
        "outputId": "c0aeac4a-ee85-41be-c16d-15e9dfa5be75"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ତ୍ରୟୋଦଶ ଶତାବ୍ଦୀରେ ନିର୍ମିତ ଏହି ସୂର୍ଯ୍ୟମନ୍ଦିର ସୂର୍ଯ୍ୟଦେବଙ୍କ ଏକ ବିଶାଳ ରଥ ଭାବେ ପରିଗଣିତ ହୋଇଥିଲା। କେତେକ ଚକା 3 ମିଟର ଚଉଡ଼ା। ସପ୍ତଦଶ ଶତାବ୍ଦୀର ପ୍ରାରମ୍ଭରେ ଜାହାଙ୍ଗୀରଙ୍କ ଦୂତ ମନ୍ଦିରକୁ ଅପବିତ୍ର କରିବା ପରେ ଏହି ମନ୍ଦିର ନଷ୍ଟ ହୋଇଯାଇଥିଲା। ଲୋକକଥା ଅନୁସାରେ ମୂର୍ତ୍ତିର ମଧ୍ୟଭାଗରେ ଗୋଟିଏ ହୀରା ଥିଲା ଯାହା ସୂର୍ଯ୍ୟକିରଣ ପ୍ରତିଫଳିତ କରୁଥିଲା। 1627 ମସିହାରେ ତତ୍କାଳୀନ ଖୋର୍ଦ୍ଧା ରାଜା ସୂର୍ଯ୍ୟମୂର୍ତ୍ତିକୁ କୋଣାର୍କରୁ ପୁରୀର ଜଗନ୍ନାଥ ମନ୍ଦିରକୁ ନେଇଯାଇଥିଲେ। ଏହି ସୂର୍ଯ୍ୟ ମନ୍ଦିର ଭାରତୀୟ ମନ୍ଦିର ସ୍ଥାପତ୍ୟର କଳିଙ୍ଗନ ବିଦ୍ୟାଳୟର ଅଟେ। ସୂର୍ଯ୍ୟମନ୍ଦିରର ପାର୍ଶ୍ୱସଜ୍ଜା ପୂର୍ବ-ପଶ୍ଚିମ ଦିଗରେ ରହିଛି। ଆଭ୍ୟନ୍ତରୀଣ ଗର୍ଭଗୃହ ବା ବିମାନ ଉପରେ ଏକ ଟାୱାର କିମ୍ବା ଶିକାରା ଥିଲା, କିନ୍ତୁ ଏହା ଉନବିଂଶ ଶତାବ୍ଦୀରେ ଧ୍ୱଂସ ହୋଇଯାଇଥିଲା। ଦର୍ଶକ ହଲ୍ ବା ଜଗମୋହନ ଏବେ ବି ଛିଡ଼ା ହୋଇ ରହିଛି ଏବଂ ସେଥିରେ ଅଧିକାଂଶ ଭଗ୍ନାବଶେଷ ରହିଛି। ନୃତ୍ୟ ଗୃହ କିମ୍ବା ନାଟମନ୍ଦିରର ଛାତ ଉଡିଯାଇଛି। ଏହା ଭଗ୍ନାବଶେଷର ପୂର୍ବ ଦିଗରେ ଏକ ଉପରକୁ ଉଠିଥିବା ପ୍ଲାଟଫର୍ମରେ ରହିଛି। ୧୫୫୯ ମସିହାରେ ମୁକୁନ୍ଦ ଗଜପତି କଟକର ସିଂହାସନ ସମ୍ଭାଳିଥିଲେ। ସେ ନିଜକୁ ଆକବରଙ୍କ ମିତ୍ର ଏବଂ ବଙ୍ଗଳାର ସୁଲତାନ ସୁଲେମାନ ଖାନ କରରାନୀଙ୍କ ଶତ୍ରୁ ଭାବେ ସାବ୍ୟସ୍ତ କରିଥିଲେ। କିଛି ଲଢ଼େଇ ପରେ ଶେଷରେ ଓଡିଶାର ପରାଜୟ ଘଟିଥିଲା। ରାଜ୍ୟର ଆଭ୍ୟନ୍ତରୀଣ ଅସ୍ଥିରତା କାରଣରୁ ମଧ୍ୟ ଏହି ହ୍ରାସ ଘଟିଛି। ୧୫୬୮ ମସିହାରେ ସୁଲତାନର ସେନାପତି କାଲାପାହାଡ଼ଙ୍କ ସେନା କୋଣାର୍କ ମନ୍ଦିରକୁ କ୍ଷତିଗ୍ରସ୍ତ କରିଥିଲେ। ବିଜୟ ସମୟରେ ଅନ୍ୟ କେତେକ ମନ୍ଦିରର କ୍ଷୟକ୍ଷତି ପାଇଁ ମଧ୍ୟ କଳାପାହାଡ଼ ଦାୟୀ ବୋଲି କୁହାଯାଏ।;Odia\n",
            "\n",
            "Odia\n",
            "ତ୍ରୟୋଦଶ ଶତାବ୍ଦୀରେ ନିର୍ମିତ ଏହି ସୂର୍ଯ୍ୟମନ୍ଦିର ସୂର୍ଯ୍ୟଦେବଙ୍କ ଏକ ବିଶାଳ ରଥ ଭାବେ ପରିଗଣିତ ହୋଇଥିଲା। କେତେକ ଚକା 3 ମିଟର ଚଉଡ଼ା। ସପ୍ତଦଶ ଶତାବ୍ଦୀର ପ୍ରାରମ୍ଭରେ ଜାହାଙ୍ଗୀରଙ୍କ ଦୂତ ମନ୍ଦିରକୁ ଅପବିତ୍ର କରିବା ପରେ ଏହି ମନ୍ଦିର ନଷ୍ଟ ହୋଇଯାଇଥିଲା। ଲୋକକଥା ଅନୁସାରେ ମୂର୍ତ୍ତିର ମଧ୍ୟଭାଗରେ ଗୋଟିଏ ହୀରା ଥିଲା ଯାହା ସୂର୍ଯ୍ୟକିରଣ ପ୍ରତିଫଳିତ କରୁଥିଲା। 1627 ମସିହାରେ ତତ୍କାଳୀନ ଖୋର୍ଦ୍ଧା ରାଜା ସୂର୍ଯ୍ୟମୂର୍ତ୍ତିକୁ କୋଣାର୍କରୁ ପୁରୀର ଜଗନ୍ନାଥ ମନ୍ଦିରକୁ ନେଇଯାଇଥିଲେ। ଏହି ସୂର୍ଯ୍ୟ ମନ୍ଦିର ଭାରତୀୟ ମନ୍ଦିର ସ୍ଥାପତ୍ୟର କଳିଙ୍ଗନ ବିଦ୍ୟାଳୟର ଅଟେ। ସୂର୍ଯ୍ୟମନ୍ଦିରର ପାର୍ଶ୍ୱସଜ୍ଜା ପୂର୍ବ-ପଶ୍ଚିମ ଦିଗରେ ରହିଛି। ଆଭ୍ୟନ୍ତରୀଣ ଗର୍ଭଗୃହ ବା ବିମାନ ଉପରେ ଏକ ଟାୱାର କିମ୍ବା ଶିକାରା ଥିଲା, କିନ୍ତୁ ଏହା ଉନବିଂଶ ଶତାବ୍ଦୀରେ ଧ୍ୱଂସ ହୋଇଯାଇଥିଲା। ଦର୍ଶକ ହଲ୍ ବା ଜଗମୋହନ ଏବେ ବି ଛିଡ଼ା ହୋଇ ରହିଛି ଏବଂ ସେଥିରେ ଅଧିକାଂଶ ଭଗ୍ନାବଶେଷ ରହିଛି। ନୃତ୍ୟ ଗୃହ କିମ୍ବା ନାଟମନ୍ଦିରର ଛାତ ଉଡିଯାଇଛି। ଏହା ଭଗ୍ନାବଶେଷର ପୂର୍ବ ଦିଗରେ ଏକ ଉପରକୁ ଉଠିଥିବା ପ୍ଲାଟଫର୍ମରେ ରହିଛି। ୧୫୫୯ ମସିହାରେ ମୁକୁନ୍ଦ ଗଜପତି କଟକର ସିଂହାସନ ସମ୍ଭାଳିଥିଲେ। ସେ ନିଜକୁ ଆକବରଙ୍କ ମିତ୍ର ଏବଂ ବଙ୍ଗଳାର ସୁଲତାନ ସୁଲେମାନ ଖାନ କରରାନୀଙ୍କ ଶତ୍ରୁ ଭାବେ ସାବ୍ୟସ୍ତ କରିଥିଲେ। କିଛି ଲଢ଼େଇ ପରେ ଶେଷରେ ଓଡିଶାର ପରାଜୟ ଘଟିଥିଲା। ରାଜ୍ୟର ଆଭ୍ୟନ୍ତରୀଣ ଅସ୍ଥିରତା କାରଣରୁ ମଧ୍ୟ ଏହି ହ୍ରାସ ଘଟିଛି। ୧୫୬୮ ମସିହାରେ ସୁଲତାନର ସେନାପତି କାଲାପାହାଡ଼ଙ୍କ ସେନା କୋଣାର୍କ ମନ୍ଦିରକୁ କ୍ଷତିଗ୍ରସ୍ତ କରିଥିଲେ। ବିଜୟ ସମୟରେ ଅନ୍ୟ କେତେକ ମନ୍ଦିରର କ୍ଷୟକ୍ଷତି ପାଇଁ ମଧ୍ୟ କଳାପାହାଡ଼ ଦାୟୀ ବୋଲି କୁହାଯାଏ।;Odia\n",
            "\n",
            "Initializing vocab and bpe\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ea129db396d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/model_files\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_load_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpostprocess_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-ea129db396d7>\u001b[0m in \u001b[0;36mmodel_load_function\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_load_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0mmodel_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"indic-en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     \u001b[0mindic2en_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindic2en_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-ea129db396d7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, expdir)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializing vocab and bpe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         self.vocabulary = read_vocabulary(\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{expdir}/vocab/vocab.SRC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         )\n\u001b[1;32m    372\u001b[0m         self.bpe = BPE(\n",
            "\u001b[0;32m/usr/lib/python3.9/codecs.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;31m# Force opening of the file in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/model_files/indic-en/vocab/vocab.SRC'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cvGtJWt3Cqak"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}